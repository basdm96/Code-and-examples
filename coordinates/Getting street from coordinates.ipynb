{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849a3cea-1db4-4dfe-9adf-4b54d7cbe817",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is my code for getting the nearest adress from coordinates. good luck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1122e274-f779-4c10-b0fe-48e32641a484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import lxml\n",
    "# --- Configuration ---\n",
    "# Set the path to your KML file\n",
    "KML_FILE_PATH = r\"C:\\Users\\basde\\Desktop\\Address from coordinates\\All Pylons - Circuits.kml\"\n",
    "\n",
    "def parse_description(description_html):\n",
    "    \"\"\"\n",
    "    Parses the HTML content of a <description> tag to extract key-value data.\n",
    "    \"\"\"\n",
    "    if not description_html:\n",
    "        return {}\n",
    "\n",
    "    # Use BeautifulSoup to handle the HTML structure within the description\n",
    "    soup = BeautifulSoup(description_html, 'html.parser')\n",
    "    \n",
    "    # Get text separated by newlines to preserve structure\n",
    "    full_text = soup.get_text(separator='\\n', strip=True)\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    # Define a mapping from the text label to the desired column name\n",
    "    key_mapping = {\n",
    "        'Pyloon nr.': 'pylon_nr',\n",
    "        'Lijn': 'line',\n",
    "        'Gemeente': 'municipality',\n",
    "        'sc': 'sc_type',\n",
    "        'x': 'lambert_x',\n",
    "        'y': 'lambert_y',\n",
    "        'PerceelNr.': 'parcel_nr',\n",
    "        'Hoogte': 'height',\n",
    "        'Technical Type': 'technical_type'\n",
    "    }\n",
    "    \n",
    "    lines = full_text.split('\\n')\n",
    "    for line in lines:\n",
    "        if ':' in line:\n",
    "            # Split only on the first colon to handle potential extra colons in the value\n",
    "            key, value = line.split(':', 1)\n",
    "            key = key.strip()\n",
    "            value = value.strip()\n",
    "            \n",
    "            if key in key_mapping:\n",
    "                column_name = key_mapping[key]\n",
    "                data[column_name] = value\n",
    "                \n",
    "    return data\n",
    "\n",
    "def parse_kml_to_dataframe(file_path):\n",
    "    \"\"\"\n",
    "    Parses a KML file and converts its placemark data into a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    print(f\"Reading KML file from: {file_path}\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            kml_content = f.read()\n",
    "\n",
    "        # Remove the default namespace definition to simplify parsing\n",
    "        kml_content_cleaned = re.sub(r'xmlns=\".*?\"', '', kml_content, count=1, flags=re.IGNORECASE)\n",
    "\n",
    "        # Use the 'lxml-xml' parser for proper XML parsing on the cleaned content\n",
    "        soup = BeautifulSoup(kml_content_cleaned, 'lxml-xml')\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{file_path}' was not found.\")\n",
    "        return None\n",
    "\n",
    "    all_pylons_data = []\n",
    "\n",
    "    # Find all Placemark elements in the document\n",
    "    placemarks = soup.find_all('Placemark')\n",
    "    print(f\"Found {len(placemarks)} placemarks.\")\n",
    "    \n",
    "    if not placemarks:\n",
    "        print(\"Warning: No <Placemark> tags were found. The resulting DataFrame will be empty.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    for placemark in placemarks:\n",
    "        # Initialize a dictionary for the current pylon\n",
    "        pylon_data = {}\n",
    "\n",
    "        # 1. Extract name (pylon_id)\n",
    "        name_tag = placemark.find('name')\n",
    "        pylon_data['pylon_id'] = name_tag.text.strip() if name_tag else None\n",
    "\n",
    "        # 2. Extract coordinates\n",
    "        coords_tag = placemark.find('coordinates')\n",
    "        if coords_tag:\n",
    "            # Split coordinates string and clean up\n",
    "            coords = [c.strip() for c in coords_tag.text.split(',')]\n",
    "            pylon_data['longitude'] = coords[0] if len(coords) > 0 else None\n",
    "            pylon_data['latitude'] = coords[1] if len(coords) > 1 else None\n",
    "            pylon_data['altitude'] = coords[2] if len(coords) > 2 else None\n",
    "        else:\n",
    "            pylon_data['longitude'] = None\n",
    "            pylon_data['latitude'] = None\n",
    "            pylon_data['altitude'] = None\n",
    "\n",
    "        # 3. Parse the complex description field\n",
    "        description_tag = placemark.find('description')\n",
    "        description_html = description_tag.text if description_tag else \"\"\n",
    "        \n",
    "        # Use the helper function to parse the description\n",
    "        description_data = parse_description(description_html)\n",
    "        pylon_data.update(description_data)\n",
    "        \n",
    "        all_pylons_data.append(pylon_data)\n",
    "\n",
    "    # Create the DataFrame from the list of dictionaries\n",
    "    df = pd.DataFrame(all_pylons_data)\n",
    "    \n",
    "    # --- Data Cleaning and Type Conversion ---\n",
    "    print(\"Cleaning and converting data types...\")\n",
    "    \n",
    "    numeric_cols = [\n",
    "        'longitude', 'latitude', 'altitude', \n",
    "        'height', 'lambert_x', 'lambert_y'\n",
    "    ]\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            # 'coerce' will turn any parsing errors into NaN\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Reorder columns for better readability\n",
    "    ordered_columns = [\n",
    "        'pylon_id', 'pylon_nr', 'line', 'municipality', 'technical_type',\n",
    "        'longitude', 'latitude', 'altitude', 'height', \n",
    "        'lambert_x', 'lambert_y', 'parcel_nr', 'sc_type'\n",
    "    ]\n",
    "    \n",
    "    # Filter to only include columns that were actually found in the file\n",
    "    final_columns = [col for col in ordered_columns if col in df.columns]\n",
    "    df = df[final_columns]\n",
    "\n",
    "    return df\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    pylons_df = parse_kml_to_dataframe(KML_FILE_PATH)\n",
    "\n",
    "    if pylons_df is not None and not pylons_df.empty:\n",
    "        print(\"\\nSuccessfully created DataFrame.\")\n",
    "        print(\"DataFrame Info:\")\n",
    "        pylons_df.info()\n",
    "        \n",
    "        print(\"\\nDataFrame Head:\")\n",
    "        print(pylons_df.head())\n",
    "    elif pylons_df is not None:\n",
    "        print(\"\\nScript finished, but the DataFrame is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b324d141-beb6-4688-9ccf-85a7649d602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import math\n",
    "import osmium\n",
    "from rtree import index\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "# Update this path to where you downloaded the 'belgium-latest.osm.pbf' file.\n",
    "BELGIUM_PBF_PATH = r\"C:\\Users\\basde\\Desktop\\Address from coordinates\\belgium-latest.osm.pbf\"\n",
    "OSRM_API_URL = \"http://router.project-osrm.org\"\n",
    "\n",
    "# --- Initialize TQDM for Pandas ---\n",
    "# This line adds a .progress_apply() method to pandas DataFrames,\n",
    "# which we will use to show a progress bar.\n",
    "tqdm.pandas(desc=\"Processing Coordinates\")\n",
    "\n",
    "print(\"Setup Complete. Libraries imported and configuration is set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9ba6c1-6d4e-43aa-8e99-c41f4f9884ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 2: CORE LOGIC AND HELPER FUNCTIONS ===\n",
    "\n",
    "class AddressIndexHandler(osmium.SimpleHandler):\n",
    "    # This class remains the same as before.\n",
    "    def __init__(self):\n",
    "        super(AddressIndexHandler, self).__init__()\n",
    "        self.addresses = []\n",
    "\n",
    "    def _process_tags(self, tags, location, element_type, element_id):\n",
    "        if 'addr:street' in tags and 'addr:housenumber' in tags:\n",
    "            self.addresses.append({\n",
    "                'type': element_type, 'id': element_id, 'lat': location.lat, 'lon': location.lon,\n",
    "                'street': tags.get('addr:street'), 'housenumber': tags.get('addr:housenumber'),\n",
    "                'city': tags.get('addr:city'), 'zip_code': tags.get('addr:postcode')\n",
    "            })\n",
    "\n",
    "    def node(self, n):\n",
    "        self._process_tags(n.tags, n.location, 'node', n.id)\n",
    "\n",
    "    def way(self, w):\n",
    "        try:\n",
    "            if not w.nodes or len(w.nodes) < 1: return\n",
    "            min_lon, min_lat, max_lon, max_lat = 181, 91, -181, -91\n",
    "            for node in w.nodes:\n",
    "                min_lon, max_lon = min(min_lon, node.lon), max(max_lon, node.lon)\n",
    "                min_lat, max_lat = min(min_lat, node.lat), max(max_lat, node.lat)\n",
    "            if min_lon > 180: return\n",
    "            center_loc = osmium.osm.Location(min_lon + (max_lon - min_lon) / 2, min_lat + (max_lat - min_lat) / 2)\n",
    "            self._process_tags(w.tags, center_loc, 'way', w.id)\n",
    "        except osmium.InvalidLocationError:\n",
    "            pass\n",
    "\n",
    "\n",
    "class LocalAddressFinder:\n",
    "    # This class remains the same as before.\n",
    "    def __init__(self, pbf_file, data_path, index_path_prefix):\n",
    "        self.pbf_file = pbf_file\n",
    "        self.data_path = data_path\n",
    "        self.index_path = index_path_prefix\n",
    "        self.address_data = []\n",
    "        \n",
    "        if os.path.exists(self.data_path) and os.path.exists(self.index_path + \".dat\"):\n",
    "            print(\"Loading pre-built index from files...\")\n",
    "            self._load_from_disk()\n",
    "            print(f\"Successfully loaded {len(self.address_data)} addresses and index.\")\n",
    "        else:\n",
    "            print(\"No pre-built index found.\")\n",
    "            self._build_from_pbf_and_save()\n",
    "\n",
    "    def _load_from_disk(self):\n",
    "        with open(self.data_path, 'rb') as f:\n",
    "            self.address_data = pickle.load(f)\n",
    "        self.idx = index.Index(self.index_path)\n",
    "\n",
    "    def _build_from_pbf_and_save(self):\n",
    "        print(f\"Building address index from '{self.pbf_file}'...\")\n",
    "        print(\"Step 1/2: Parsing OSM data file (this may take a minute)...\")\n",
    "        handler = AddressIndexHandler()\n",
    "        handler.apply_file(self.pbf_file, locations=True)\n",
    "        self.address_data = handler.addresses\n",
    "        \n",
    "        print(f\"Saving {len(self.address_data)} addresses to '{self.data_path}'...\")\n",
    "        with open(self.data_path, 'wb') as f:\n",
    "            pickle.dump(self.address_data, f)\n",
    "        \n",
    "        print(f\"Step 2/2: Indexing addresses and saving to '{self.index_path}.dat/idx'...\")\n",
    "        p = index.Property()\n",
    "        p.overwrite = True\n",
    "        self.idx = index.Index(self.index_path, properties=p)\n",
    "        \n",
    "        for i, addr in enumerate(self.address_data):\n",
    "            self.idx.insert(i, (addr['lon'], addr['lat'], addr['lon'], addr['lat']))\n",
    "        \n",
    "        self.idx.close()\n",
    "        print(\"Index build and save complete.\")\n",
    "\n",
    "    def find_candidates(self, lat, lon, radius_m=2000):\n",
    "        lat_offset = radius_m / 111111.0\n",
    "        lon_offset = radius_m / (111111.0 * math.cos(math.radians(lat)))\n",
    "        search_bbox = (lon - lon_offset, lat - lat_offset, lon + lon_offset, lat + lat_offset)\n",
    "        candidate_indices = self.idx.intersection(search_bbox)\n",
    "        return [self.address_data[i] for i in candidate_indices]\n",
    "\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculates the straight-line distance between two points on Earth.\"\"\"\n",
    "    R = 6371000  # Radius of Earth in meters\n",
    "    phi1 = math.radians(lat1)\n",
    "    phi2 = math.radians(lat2)\n",
    "    delta_phi = math.radians(lat2 - lat1)\n",
    "    delta_lambda = math.radians(lon2 - lon1)\n",
    "\n",
    "    a = math.sin(delta_phi / 2)**2 + math.cos(phi1) * math.cos(phi2) * math.sin(delta_lambda / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "    return R * c\n",
    "\n",
    "def main_orchestrator(lat, lon, address_finder):\n",
    "    \"\"\"\n",
    "    Finds the geometrically closest address to a point by calculating\n",
    "    the straight-line distance.\n",
    "    \"\"\"\n",
    "    result_template = {'street': None, 'housenumber': None, 'city': None, \n",
    "                       'zip_code': None, 'straight_line_distance_m': None}\n",
    "    \n",
    "    # Step 1: Find all potential candidates in a radius\n",
    "    candidates = address_finder.find_candidates(lat, lon)\n",
    "    if not candidates:\n",
    "        return result_template\n",
    "\n",
    "    # Step 2: Loop through candidates to find the one with the minimum straight-line distance\n",
    "    best_candidate = None\n",
    "    min_distance = float('inf')\n",
    "\n",
    "    for candidate in candidates:\n",
    "        distance = haversine_distance(lat, lon, candidate['lat'], candidate['lon'])\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            best_candidate = candidate\n",
    "    \n",
    "    # Step 3: If a best candidate was found, populate the results\n",
    "    if best_candidate:\n",
    "        result_template.update({\n",
    "            'street': best_candidate.get('street'),\n",
    "            'housenumber': best_candidate.get('housenumber'),\n",
    "            'city': best_candidate.get('city'),\n",
    "            'zip_code': best_candidate.get('zip_code'),\n",
    "            'straight_line_distance_m': min_distance\n",
    "        })\n",
    "        \n",
    "    return result_template\n",
    "\n",
    "print(\"Core logic functions and classes defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa32914e-f86f-46dd-9970-230fee96e5c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from rtree import index\n",
    "import os\n",
    "# === CELL 3: INITIALIZE ADDRESS FINDER (Builds or Loads Index) ===\n",
    "SAVED_DATA_PATH = \"address_data.pkl\"\n",
    "SAVED_INDEX_PATH_PREFIX = \"address_index\"\n",
    "# This will now check for \"address_data.pkl\" and \"address_index.dat/idx\".\n",
    "# If they exist, it will load them. If not, it will build them from the PBF\n",
    "# file and save them for the next time.\n",
    "try:\n",
    "    address_finder = LocalAddressFinder(BELGIUM_PBF_PATH, \n",
    "                                        data_path=SAVED_DATA_PATH, \n",
    "                                        index_path_prefix=SAVED_INDEX_PATH_PREFIX)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff57d15e-1ec2-48cc-bc72-8594716d8a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === ONE-TIME CELL TO SAVE THE EXISTING IN-MEMORY INDEX ===\n",
    "\n",
    "# import pickle\n",
    "# from rtree import index\n",
    "# import os\n",
    "\n",
    "# print(\"Saving the data from the existing 'address_finder' object...\")\n",
    "\n",
    "# # 1. Define the filenames (must match the new code)\n",
    "# SAVED_DATA_PATH = \"address_data.pkl\"\n",
    "# SAVED_INDEX_PATH_PREFIX = \"address_index\"\n",
    "\n",
    "# # 2. Save the address_data list using pickle\n",
    "# print(f\"Step 1/2: Saving {len(address_finder.address_data)} addresses to '{SAVED_DATA_PATH}'...\")\n",
    "# with open(SAVED_DATA_PATH, 'wb') as f:\n",
    "#     pickle.dump(address_finder.address_data, f)\n",
    "# print(\"...address data saved successfully.\")\n",
    "\n",
    "# # 3. Create a new, persistent R-tree index and populate it from the existing data\n",
    "# print(f\"Step 2/2: Saving the spatial index to '{SAVED_INDEX_PATH_PREFIX}.dat/idx'...\")\n",
    "# p = index.Property()\n",
    "# p.overwrite = True # Overwrite index files if they already exist\n",
    "# persistent_idx = index.Index(SAVED_INDEX_PATH_PREFIX, properties=p)\n",
    "\n",
    "# for i, addr in enumerate(address_finder.address_data):\n",
    "#     persistent_idx.insert(i, (addr['lon'], addr['lat'], addr['lon'], addr['lat']))\n",
    "\n",
    "# persistent_idx.close() # Important: This flushes the index to disk\n",
    "# print(\"...spatial index saved successfully.\")\n",
    "\n",
    "# print(\"\\nSave complete! You can now use the new code to load this index instantly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feb4557-a522-4fe0-9dcf-763361b352da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 5: RUN THE PROCESSING ===\n",
    "\n",
    "# We use .progress_apply() on your pylons_df DataFrame.\n",
    "# This will find the closest reachable address for each pylon.\n",
    "results = pylons_df.progress_apply(\n",
    "    lambda row: main_orchestrator(row['latitude'], row['longitude'], address_finder), \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f47aa0d-c225-4de5-922a-22927d142646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 6: FORMAT AND DISPLAY FINAL RESULTS ===\n",
    "\n",
    "# Convert the Series of dictionaries into a new DataFrame\n",
    "results_df = pd.DataFrame(results.tolist())\n",
    "\n",
    "# Concatenate the new address columns with your original pylons_df\n",
    "pylons_df_enriched = pd.concat([pylons_df.reset_index(drop=True), results_df], axis=1)\n",
    "\n",
    "print(\"Processing complete. Enriched pylons_df:\")\n",
    "display(pylons_df_enriched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c41abef-2957-439f-911d-f2d2365b35d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# --- Data Cleaning Steps ---\n",
    "# It's still a good practice to clean the column first.\n",
    "pylons_df_enriched['pylon_id_cleaned'] = pylons_df_enriched['pylon_id'].astype(str).str.strip()\n",
    "\n",
    "\n",
    "# --- Splitting the Column ---\n",
    "# Use .str.split() with expand=True to create a new DataFrame from the split parts.\n",
    "split_df = pylons_df_enriched['pylon_id_cleaned'].str.split('_', expand=True)\n",
    "\n",
    "\n",
    "# --- Assigning to New Columns ---\n",
    "# Assign the first and second column from the split to 'UGE' and 'Nr.'\n",
    "pylons_df_enriched['UGE'] = split_df[0]\n",
    "pylons_df_enriched['Nr.'] = split_df[1]\n",
    "\n",
    "\n",
    "# --- Identifying Incorrect Formats ---\n",
    "# A format is considered incorrect if it doesn't split into exactly two parts.\n",
    "# This happens if there is no underscore (split_df[1] will be None)\n",
    "# or if there is more than one underscore (split_df[2] will not be None).\n",
    "incorrect_mask = (split_df[1].isnull()) | (split_df[2].notnull())\n",
    "incorrect_format_df = pylons_df_enriched[incorrect_mask]\n",
    "\n",
    "\n",
    "# We can drop the intermediate cleaned column if it's no longer needed.\n",
    "pylons_df_enriched = pylons_df_enriched.drop(columns=['pylon_id_cleaned'])\n",
    "\n",
    "\n",
    "print(\"DataFrame with new 'UGE' and 'Nr.' columns using split:\")\n",
    "print(pylons_df_enriched)\n",
    "print(\"\\nRows with incorrect format (did not split into 2 parts):\")\n",
    "print(incorrect_format_df.drop(columns=['UGE', 'Nr.'], errors='ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6a7260-8659-4586-817c-2c5f8c8062fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bae2d2-5571-4aa7-b079-49cfbc2bfdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the name for your output file\n",
    "output_filename = 'lines_data2.csv'\n",
    "\n",
    "# 2. Use the .to_csv() method to save the DataFrame\n",
    "#    We use index=False because we typically don't need to save the\n",
    "#    DataFrame's index numbers to the CSV file.\n",
    "pylons_df_enriched.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"DataFrame has been successfully saved to '{output_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f655ff3a-1bea-4c80-80cd-7324ec9432f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
