{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a83ca33-c4af-44d0-bfc1-2e77018dc26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4778aecf-73cb-4404-8cdc-01041192801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classification dataset\n",
    "dft = pd.read_csv(r\"C:\\Users\\basde\\Documents\\GitHub\\Code-and-examples\\Projects\\Binary Classification with a Bank Dataset\\train.csv\")\n",
    "dfo = pd.read_csv(r\"C:\\Users\\basde\\Documents\\GitHub\\Code-and-examples\\Projects\\Binary Classification with a Bank Dataset\\bank-full.csv\", delimiter=';')\n",
    "dfo['y'] = dfo['y'].map({'no' : 0, 'yes': 1})\n",
    "# df = pd.concat([dft, dfo])\n",
    "y = dfo['y']\n",
    "dfo = dfo.drop('y', axis=1)\n",
    "dfo = dfo.drop(['month', 'day', 'job','marital'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e0e6daf-4a4b-4ea6-81e8-fede0d104152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "education\n",
       "secondary    23202\n",
       "tertiary     13301\n",
       "primary       6851\n",
       "unknown       1857\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfo['education'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80419a82-90ce-450f-ab82-5505c7767502",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo['default'] = pd.get_dummies(dfo['default'], drop_first=True, dtype=int)\n",
    "dfo['housing'] = pd.get_dummies(dfo['housing'], drop_first=True, dtype=int)\n",
    "dfo['loan'] = pd.get_dummies(dfo['loan'], drop_first=True, dtype=int)\n",
    "# dfo['poutcome'].map({'failure': '0', 'other' : '0', 'unknown' : '0', 'succes' : '1'}).astype('int')\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "category_order = ['primary', 'secondary', 'unknown', 'tertiary']\n",
    "ordinal_encoder = OrdinalEncoder(categories=[category_order])\n",
    "dfo['education'] = ordinal_encoder.fit_transform(dfo[['education']])\n",
    "dfo['contact'] = dfo['contact'].map({'telephone': 'cellular', 'unknown' : 'unknown', 'telephone': 'telephone'})\n",
    "dfo['contact'] = pd.get_dummies(dfo['contact'], drop_first=True, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a57e6f-9253-45da-9061-3aff95b21f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfo.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29e4e136-5974-4ba6-985c-335623f2133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import set_config\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "set_config(transform_output=\"pandas\")\n",
    "# Custom transformer for manual mapping of categorical values\n",
    "class ColumnValueMapper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Applies a specified mapping to a column.\n",
    "    Values not in the mapping are left unchanged.\n",
    "    \"\"\"\n",
    "    def __init__(self, mapping):\n",
    "        self.mapping = mapping\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_out = X.copy()\n",
    "        # Get the first (and only) column name to apply the mapping\n",
    "        col_name = X_out.columns[0]\n",
    "        X_out[col_name] = X_out[col_name].replace(self.mapping)\n",
    "        return X_out\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return input_features\n",
    "\n",
    "# a. Specify the feature to apply transformations on\n",
    "feature_to_transform = 'poutcome'\n",
    "\n",
    "# b. Define the classification models to evaluate\n",
    "models = {\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "    \"CatBoost\": CatBoostClassifier(random_state=42, verbose=0)\n",
    "}\n",
    "\n",
    "# c. Define the transformation pipelines to test on the specified feature\n",
    "transformation_pipelines = {\n",
    "    'OneHotEncoder': ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('encoder', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False))\n",
    "            ]), [feature_to_transform])\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    ),\n",
    "    'OrdinalEncoder': ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('encoder', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "            ]), [feature_to_transform])\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    ),\n",
    "    'ManualMap_OneHotEncoder': ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('encoder', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('mapper', ColumnValueMapper(mapping={'failure': '0', 'other' : '0', 'unknown' : '0', 'succes' : '1'})),\n",
    "                ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "            ]), [feature_to_transform])\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "}\n",
    "\n",
    "# d. Define the classification scoring metrics\n",
    "scoring_metrics = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1_score': 'f1_weighted',\n",
    "    'precision': 'precision_weighted',\n",
    "    'recall': 'recall_weighted'\n",
    "}\n",
    "\n",
    "# e. Define the cross-validation strategy\n",
    "cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c293cf4-e09f-4180-b5a0-cd919777ce60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating Model: LightGBM ---\n",
      "[LightGBM] [Info] Number of positive: 4198, number of negative: 31970\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004112 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 925\n",
      "[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116069 -> initscore=-2.030190\n",
      "[LightGBM] [Info] Start training from score -2.030190\n",
      "[LightGBM] [Info] Number of positive: 4279, number of negative: 31890\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 926\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.118306 -> initscore=-2.008573\n",
      "[LightGBM] [Info] Start training from score -2.008573\n",
      "[LightGBM] [Info] Number of positive: 4184, number of negative: 31985\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004371 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 924\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.115679 -> initscore=-2.033999\n",
      "[LightGBM] [Info] Start training from score -2.033999\n",
      "[LightGBM] [Info] Number of positive: 4269, number of negative: 31900\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 924\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.118029 -> initscore=-2.011226\n",
      "[LightGBM] [Info] Start training from score -2.011226\n",
      "[LightGBM] [Info] Number of positive: 4226, number of negative: 31943\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004164 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 926\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116840 -> initscore=-2.022697\n",
      "[LightGBM] [Info] Start training from score -2.022697\n",
      "--- feature dropped: LightGBM ---\n",
      "[LightGBM] [Info] Number of positive: 4198, number of negative: 31970\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004546 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 931\n",
      "[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116069 -> initscore=-2.030190\n",
      "[LightGBM] [Info] Start training from score -2.030190\n",
      "[LightGBM] [Info] Number of positive: 4279, number of negative: 31890\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005566 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 932\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.118306 -> initscore=-2.008573\n",
      "[LightGBM] [Info] Start training from score -2.008573\n",
      "[LightGBM] [Info] Number of positive: 4184, number of negative: 31985\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004187 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 930\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.115679 -> initscore=-2.033999\n",
      "[LightGBM] [Info] Start training from score -2.033999\n",
      "[LightGBM] [Info] Number of positive: 4269, number of negative: 31900\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004928 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 930\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.118029 -> initscore=-2.011226\n",
      "[LightGBM] [Info] Start training from score -2.011226\n",
      "[LightGBM] [Info] Number of positive: 4226, number of negative: 31943\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004224 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 932\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116840 -> initscore=-2.022697\n",
      "[LightGBM] [Info] Start training from score -2.022697\n",
      "--- tranformation: OneHotEncoder ---\n",
      "[LightGBM] [Info] Number of positive: 4198, number of negative: 31970\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004478 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 929\n",
      "[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116069 -> initscore=-2.030190\n",
      "[LightGBM] [Info] Start training from score -2.030190\n",
      "[LightGBM] [Info] Number of positive: 4279, number of negative: 31890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005368 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 930\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.118306 -> initscore=-2.008573\n",
      "[LightGBM] [Info] Start training from score -2.008573\n",
      "[LightGBM] [Info] Number of positive: 4184, number of negative: 31985\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004097 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 928\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.115679 -> initscore=-2.033999\n",
      "[LightGBM] [Info] Start training from score -2.033999\n",
      "[LightGBM] [Info] Number of positive: 4269, number of negative: 31900\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003856 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 928\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.118029 -> initscore=-2.011226\n",
      "[LightGBM] [Info] Start training from score -2.011226\n",
      "[LightGBM] [Info] Number of positive: 4226, number of negative: 31943\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004280 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 930\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116840 -> initscore=-2.022697\n",
      "[LightGBM] [Info] Start training from score -2.022697\n",
      "--- tranformation: OrdinalEncoder ---\n",
      "[LightGBM] [Info] Number of positive: 4198, number of negative: 31970\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005343 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 929\n",
      "[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116069 -> initscore=-2.030190\n",
      "[LightGBM] [Info] Start training from score -2.030190\n",
      "[LightGBM] [Info] Number of positive: 4279, number of negative: 31890\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004163 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 930\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.118306 -> initscore=-2.008573\n",
      "[LightGBM] [Info] Start training from score -2.008573\n",
      "[LightGBM] [Info] Number of positive: 4184, number of negative: 31985\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003753 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 928\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.115679 -> initscore=-2.033999\n",
      "[LightGBM] [Info] Start training from score -2.033999\n",
      "[LightGBM] [Info] Number of positive: 4269, number of negative: 31900\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004626 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 928\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.118029 -> initscore=-2.011226\n",
      "[LightGBM] [Info] Start training from score -2.011226\n",
      "[LightGBM] [Info] Number of positive: 4226, number of negative: 31943\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004325 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 930\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116840 -> initscore=-2.022697\n",
      "[LightGBM] [Info] Start training from score -2.022697\n",
      "--- tranformation: ManualMap_OneHotEncoder ---\n",
      "--- Evaluating Model: CatBoost ---\n",
      "--- feature dropped: CatBoost ---\n",
      "--- tranformation: OneHotEncoder ---\n",
      "--- tranformation: OrdinalEncoder ---\n",
      "--- tranformation: ManualMap_OneHotEncoder ---\n"
     ]
    }
   ],
   "source": [
    "# This DataFrame will hold all results for final comparison\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# --- Main Loop ---\n",
    "for model_name, model in models.items():\n",
    "    print(f\"--- Evaluating Model: {model_name} ---\")\n",
    "    model_results = {}\n",
    "\n",
    "    # a. \"Feature Dropped\" Evaluation\n",
    "    # Dynamically drops the feature specified in cell 3\n",
    "    X_dropped = X.drop(columns=[feature_to_transform])\n",
    "    feature_dropped_scores = cross_validate(\n",
    "        model, X_dropped, y, cv=cv_strategy,\n",
    "        scoring=scoring_metrics, return_train_score=True\n",
    "    )\n",
    "    model_results['Feature Dropped'] = {\n",
    "        'Train F1-Score': feature_dropped_scores['train_f1_score'].mean(),\n",
    "        'CV F1-Score': feature_dropped_scores['test_f1_score'].mean(),\n",
    "        'CV Accuracy': feature_dropped_scores['test_accuracy'].mean()\n",
    "    }\n",
    "    print(f\"--- feature dropped: {model_name} ---\")\n",
    "\n",
    "    # c. Transformation Pipelines Evaluation\n",
    "    # Iterates through the ColumnTransformer pipelines defined in cell 3\n",
    "    for tech_name, preprocessor in transformation_pipelines.items():\n",
    "        full_pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        scores = cross_validate(\n",
    "            full_pipeline, X, y, cv=cv_strategy,\n",
    "            scoring=scoring_metrics, return_train_score=True\n",
    "        )\n",
    "        model_results[tech_name] = {\n",
    "            'Train F1-Score': scores['train_f1_score'].mean(),\n",
    "            'CV F1-Score': scores['test_f1_score'].mean(),\n",
    "            'CV Accuracy': scores['test_accuracy'].mean()\n",
    "        }\n",
    "        print(f\"--- tranformation: {tech_name} ---\")\n",
    "    # d. Consolidate and store results\n",
    "    temp_df = pd.DataFrame.from_dict(model_results, orient='index')\n",
    "    temp_df['Model'] = model_name\n",
    "    all_results = pd.concat([all_results, temp_df])\n",
    "\n",
    "# e. Final processing for the results table\n",
    "all_results.reset_index(inplace=True)\n",
    "all_results.rename(columns={'index': 'Preprocessing Technique'}, inplace=True)\n",
    "all_results['Generalization'] = all_results['CV F1-Score'] / all_results['Train F1-Score']\n",
    "all_results = all_results.sort_values(by='CV F1-Score', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "488e7202-c308-46ad-8035-3a282447c830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing Technique</th>\n",
       "      <th>CV F1-Score</th>\n",
       "      <th>CV Accuracy</th>\n",
       "      <th>Train F1-Score</th>\n",
       "      <th>Generalization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>ManualMap_OneHotEncoder</td>\n",
       "      <td>0.893398</td>\n",
       "      <td>0.902856</td>\n",
       "      <td>0.912014</td>\n",
       "      <td>0.979588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.893398</td>\n",
       "      <td>0.902922</td>\n",
       "      <td>0.912159</td>\n",
       "      <td>0.979432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.892881</td>\n",
       "      <td>0.902258</td>\n",
       "      <td>0.912397</td>\n",
       "      <td>0.978610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>ManualMap_OneHotEncoder</td>\n",
       "      <td>0.891275</td>\n",
       "      <td>0.901197</td>\n",
       "      <td>0.923285</td>\n",
       "      <td>0.965330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.891137</td>\n",
       "      <td>0.901219</td>\n",
       "      <td>0.923935</td>\n",
       "      <td>0.964502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.890415</td>\n",
       "      <td>0.900511</td>\n",
       "      <td>0.923801</td>\n",
       "      <td>0.963860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>Feature Dropped</td>\n",
       "      <td>0.889643</td>\n",
       "      <td>0.899560</td>\n",
       "      <td>0.909068</td>\n",
       "      <td>0.978632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>Feature Dropped</td>\n",
       "      <td>0.887939</td>\n",
       "      <td>0.898542</td>\n",
       "      <td>0.919940</td>\n",
       "      <td>0.965214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  Preprocessing Technique  CV F1-Score  CV Accuracy  \\\n",
       "3  LightGBM  ManualMap_OneHotEncoder     0.893398     0.902856   \n",
       "2  LightGBM           OrdinalEncoder     0.893398     0.902922   \n",
       "1  LightGBM            OneHotEncoder     0.892881     0.902258   \n",
       "7  CatBoost  ManualMap_OneHotEncoder     0.891275     0.901197   \n",
       "6  CatBoost           OrdinalEncoder     0.891137     0.901219   \n",
       "5  CatBoost            OneHotEncoder     0.890415     0.900511   \n",
       "0  LightGBM          Feature Dropped     0.889643     0.899560   \n",
       "4  CatBoost          Feature Dropped     0.887939     0.898542   \n",
       "\n",
       "   Train F1-Score  Generalization  \n",
       "3        0.912014        0.979588  \n",
       "2        0.912159        0.979432  \n",
       "1        0.912397        0.978610  \n",
       "7        0.923285        0.965330  \n",
       "6        0.923935        0.964502  \n",
       "5        0.923801        0.963860  \n",
       "0        0.909068        0.978632  \n",
       "4        0.919940        0.965214  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reorder columns for a more logical presentation\n",
    "final_columns_order = [\n",
    "    'Model',\n",
    "    'Preprocessing Technique',\n",
    "    'CV F1-Score',\n",
    "    'CV Accuracy',\n",
    "    'Train F1-Score',\n",
    "    'Generalization'\n",
    "]\n",
    "all_results = all_results[final_columns_order]\n",
    "\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4be208fa-ad43-40a2-9f48-6a245707584d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1450552359.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mLightGBM\tManualMap_OneHotEncoder\t0.893398\t0.902856\t0.912014\t0.979588\u001b[39m\n            \t^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "LightGBM\tManualMap_OneHotEncoder\t0.893398\t0.902856\t0.912014\t0.979588\n",
    "6\tLightGBM\tOrdinalEncoder\t0.893398\t0.902922\t0.912159\t0.979432\n",
    "5\tLightGBM\tOneHotEncoder\t0.892881\t0.902258\t0.912397\t0.978610\n",
    "\n",
    "LightGBM\tOrdinalEncoder\t0.893530\t0.902856\t0.911901\t0.979854\n",
    "7\tLightGBM\tManualMap_OneHotEncoder\t0.893398\t0.902856\t0.912014\t0.979588\n",
    "5\tLightGBM\tOneHotEncoder\t0.892760\t0.902325\t0.912347\t0.978531"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
