{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eeca7eb-45ce-4249-bc3e-e60bb18dba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe6fdb43-ff30-4577-83ea-714eb6fd1d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\"C:\\Users\\basde\\OneDrive\\Documenten\\GitHub\\Titanic\\train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22ec4ccc-caa5-4d92-9d94-a145bd1c09c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Sex'] = train['Sex'].map({'male': 0, 'female': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c01dc54-1025-483d-8e61-371719d8b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Fare'] = np.log1p(train['Fare'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3fcca42-ad92-4f5e-ae24-23704909e452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def add_title_column(df):\n",
    "    \"\"\"\n",
    "    Cleans the 'Name' column and adds a 'Title' column directly \n",
    "    to the input DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to modify in-place.\n",
    "    \"\"\"\n",
    "    # 1. Clean the Name column (remove text in parentheses and quotes)\n",
    "    # This series is temporary and will be used to build the 'Title'\n",
    "    cleaned_names = df['Name'].apply(lambda x: re.sub(r'\\([^)]*\\)', '', x).strip())\n",
    "    cleaned_names = cleaned_names.str.replace(r'\"[^\"]*\"', '', regex=True).str.strip()\n",
    "\n",
    "    # 2. Extract the part of the name after the comma\n",
    "    name_part = cleaned_names.str.split(',').str.get(1)\n",
    "\n",
    "    # 3. Extract the Title from the remaining part of the name\n",
    "    extracted_title = name_part.str.split('.').str.get(0).str.strip()\n",
    "    \n",
    "    # 4. Standardize the common titles\n",
    "    title_mapping = {\n",
    "        'Mlle': 'Miss',\n",
    "        'Ms': 'Miss',\n",
    "        'Mme': 'Mrs'\n",
    "    }\n",
    "    extracted_title = extracted_title.replace(title_mapping)\n",
    "\n",
    "    # 5. Define a list of common titles\n",
    "    common_titles = ['Mr', 'Miss', 'Mrs', 'Master']\n",
    "\n",
    "    # 6. Create the 'Title' column directly on the DataFrame\n",
    "    # Categorize any title not in common_titles as 'Rare'\n",
    "    df['Titel'] = extracted_title.apply(lambda x: x if x in common_titles else 'Rare')\n",
    "    df.drop('Name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a22d192-b41a-4f87-ba32-dd70a02af0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_title_column(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5976ab6c-a9bb-4d5d-b9ad-3db87a28b6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['Survived']\n",
    "X_train = train.drop(['Survived','Cabin','Embarked', 'Ticket'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15fc1ade-6b03-471a-b7bf-fe997a40de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import warnings\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c2e35b7-1317-4b60-b961-72bcec514130",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedMedianImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A custom transformer to impute missing 'Age' values based on the\n",
    "    median of specified groups.\n",
    "    \"\"\"\n",
    "    def __init__(self, group_cols):\n",
    "        # Initialize with the column(s) to group by (e.g., ['Titel', 'Pclass'])\n",
    "        self.group_cols = group_cols\n",
    "        # Dictionary to store the median 'Age' for each group\n",
    "        self.medians = {}\n",
    "        # A fallback median for the entire dataset\n",
    "        self.global_median = 0\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Learns the median 'Age' from the training data for each group.\"\"\"\n",
    "        X_ = X.copy()\n",
    "        \n",
    "        # Calculate and store the median 'Age' for each group\n",
    "        if self.group_cols:\n",
    "            self.medians = X_.groupby(self.group_cols)['Age'].median()\n",
    "        \n",
    "        # Calculate and store the overall median 'Age' as a fallback\n",
    "        self.global_median = X_['Age'].median()\n",
    "        \n",
    "        # This is standard practice in scikit-learn\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Applies the learned medians to fill missing 'Age' values.\"\"\"\n",
    "        X_ = X.copy()\n",
    "        \n",
    "        # Impute 'Age' using the median of each group\n",
    "        if self.group_cols:\n",
    "            X_['Age'] = X_.groupby(self.group_cols)['Age'].transform(lambda x: x.fillna(x.median()))\n",
    "        \n",
    "        # Fill any remaining missing 'Age' values with the global median\n",
    "        # (This handles groups in the test set that weren't in the train set)\n",
    "        X_['Age'] = X_['Age'].fillna(self.global_median)\n",
    "        \n",
    "        # Drop non-numeric columns to ensure the output is ready for a model\n",
    "        X_ = X_.select_dtypes(include=np.number)\n",
    "        \n",
    "        return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73ee2d98-0377-485d-98d1-61617a264acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "X_train['Titel'] = le.fit_transform(X_train['Titel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce06a3da-3892-4db4-97f7-523c64b04df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Setup\n",
    "# a. Define columns to process\n",
    "columns_to_process = ['Age']\n",
    "\n",
    "# b. Define cross-validation strategy\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# c. Define evaluation models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=2000, random_state=42),\n",
    "    \"Ridge Classifier\": RidgeClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier()\n",
    "}\n",
    "\n",
    "scoring_metrics = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision_weighted', \n",
    "    'recall': 'recall_weighted',\n",
    "    'f1_score': 'f1_weighted'\n",
    "}\n",
    "\n",
    "# b. Define cross-validation strategy\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# d. Define techniques dictionary\n",
    "techniques = {}\n",
    "grouping_features = ['Pclass', 'Titel'] \n",
    "for i in range(1, len(grouping_features) + 1):\n",
    "    for combo in itertools.combinations(grouping_features, i):\n",
    "        combo_list = list(combo)\n",
    "        name = f\"Median Impute by {'_&_'.join(combo_list)}\"\n",
    "        techniques[name] = GroupedMedianImputer(group_cols=combo_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b29072ee-1cca-478c-8a57-0cd0483e7cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating Model: Logistic Regression ---\n",
      "\n",
      "--- Evaluating Model: Ridge Classifier ---\n",
      "\n",
      "--- Evaluating Model: AdaBoost ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_train = pd.DataFrame()\n",
    "for model_name, model in models.items():\n",
    "    print(f\"--- Evaluating Model: {model_name} ---\\n\")\n",
    "    results = {}\n",
    "    \n",
    "\n",
    "    \n",
    "    # b. Baseline Evaluation\n",
    "    # For a fair baseline, we'll impute 'Age' with the global median\n",
    "    train_baseline = X_train.copy()\n",
    "    train_baseline['Age'] = train_baseline['Age'].fillna(X_train['Age'].median())\n",
    "    baseline_scores = cross_validate(\n",
    "        model, \n",
    "        X=train_baseline.drop('Titel', axis=1), \n",
    "        y= y, \n",
    "        cv=cv, \n",
    "        scoring=scoring_metrics, \n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    results['Baseline'] = {\n",
    "        'Train accuracy Mean': baseline_scores['train_accuracy'].mean(),\n",
    "        'CV accuracy Mean': baseline_scores['test_accuracy'].mean(),\n",
    "        'train_f1_Mean': baseline_scores['train_f1_score'].mean(),\n",
    "        'CV_f1_Mean': baseline_scores['test_f1_score'].mean()\n",
    "    }\n",
    "\n",
    " \n",
    "    for name, imputer in techniques.items():\n",
    "        # The imputer is the first step\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('imputer', imputer),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        \n",
    "        # Perform cross-validation on the pipeline\n",
    "        # The full train is passed to the pipeline\n",
    "        scores = cross_validate(pipeline, X_train, y, cv=cv, scoring=scoring_metrics, return_train_score=True)\n",
    "        \n",
    "        results[name] = {\n",
    "        'Train accuracy Mean': scores['train_accuracy'].mean(),\n",
    "        'CV accuracy Mean': scores['test_accuracy'].mean(),\n",
    "        'train_f1_Mean': scores['train_f1_score'].mean(),\n",
    "        'CV_f1_Mean': scores['test_f1_score'].mean()\n",
    "        }\n",
    "\n",
    "    # d. Feature Dropped Evaluation*\n",
    "    train_dropped = X_train.drop(columns=columns_to_process)\n",
    "    dropped_scores = cross_validate(model, train_dropped, y, cv=cv, scoring=scoring_metrics, return_train_score=True)\n",
    "    results['Feature Dropped'] = {\n",
    "        'Train accuracy Mean': dropped_scores['train_accuracy'].mean(),\n",
    "        'CV accuracy Mean': dropped_scores['test_accuracy'].mean(),\n",
    "        'train_f1_Mean': dropped_scores['train_f1_score'].mean(),\n",
    "        'CV_f1_Mean': dropped_scores['test_f1_score'].mean()\n",
    "        }\n",
    "    \n",
    "    model_results_train = pd.DataFrame.from_dict(results, orient='index')\n",
    "    model_results_train.insert(loc=0, column='Model', value = 'a')\n",
    "    model_results_train['Model'] = model_name\n",
    "    results_train = pd.concat([results_train, model_results_train])\n",
    "    results_train['Overfitting Score'] = results_train['train_f1_Mean'] - results_train['CV_f1_Mean']\n",
    "    results_train['Generalization Ratio'] = results_train['CV_f1_Mean'] / results_train['train_f1_Mean']\n",
    "    results_train = results_train.sort_values(by='CV_f1_Mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5066abab-e747-4b00-9348-95d21e29f5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train accuracy Mean</th>\n",
       "      <th>CV accuracy Mean</th>\n",
       "      <th>train_f1_Mean</th>\n",
       "      <th>CV_f1_Mean</th>\n",
       "      <th>Overfitting Score</th>\n",
       "      <th>Generalization Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Median Impute by Pclass_&amp;_Titel</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.828843</td>\n",
       "      <td>0.820438</td>\n",
       "      <td>0.827523</td>\n",
       "      <td>0.818651</td>\n",
       "      <td>0.008872</td>\n",
       "      <td>0.989278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Median Impute by Titel</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.827160</td>\n",
       "      <td>0.818191</td>\n",
       "      <td>0.825635</td>\n",
       "      <td>0.816315</td>\n",
       "      <td>0.009320</td>\n",
       "      <td>0.988712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Median Impute by Pclass</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.815944</td>\n",
       "      <td>0.823637</td>\n",
       "      <td>0.815027</td>\n",
       "      <td>0.008610</td>\n",
       "      <td>0.989546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature Dropped</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.827720</td>\n",
       "      <td>0.808085</td>\n",
       "      <td>0.826484</td>\n",
       "      <td>0.806241</td>\n",
       "      <td>0.020243</td>\n",
       "      <td>0.975507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.814534</td>\n",
       "      <td>0.805831</td>\n",
       "      <td>0.813776</td>\n",
       "      <td>0.804658</td>\n",
       "      <td>0.009117</td>\n",
       "      <td>0.988796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Median Impute by Pclass</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.807237</td>\n",
       "      <td>0.799084</td>\n",
       "      <td>0.805415</td>\n",
       "      <td>0.797176</td>\n",
       "      <td>0.008239</td>\n",
       "      <td>0.989770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Median Impute by Pclass</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.801627</td>\n",
       "      <td>0.796874</td>\n",
       "      <td>0.799391</td>\n",
       "      <td>0.794720</td>\n",
       "      <td>0.004672</td>\n",
       "      <td>0.994156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature Dropped</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.797138</td>\n",
       "      <td>0.796887</td>\n",
       "      <td>0.794723</td>\n",
       "      <td>0.794236</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.999388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Median Impute by Pclass_&amp;_Titel</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.800786</td>\n",
       "      <td>0.795744</td>\n",
       "      <td>0.798787</td>\n",
       "      <td>0.793641</td>\n",
       "      <td>0.005146</td>\n",
       "      <td>0.993558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Median Impute by Pclass_&amp;_Titel</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.806957</td>\n",
       "      <td>0.795706</td>\n",
       "      <td>0.804929</td>\n",
       "      <td>0.793166</td>\n",
       "      <td>0.011763</td>\n",
       "      <td>0.985386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.796297</td>\n",
       "      <td>0.793503</td>\n",
       "      <td>0.794436</td>\n",
       "      <td>0.791612</td>\n",
       "      <td>0.002824</td>\n",
       "      <td>0.996445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.798542</td>\n",
       "      <td>0.791237</td>\n",
       "      <td>0.797590</td>\n",
       "      <td>0.790443</td>\n",
       "      <td>0.007146</td>\n",
       "      <td>0.991040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Median Impute by Titel</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.801347</td>\n",
       "      <td>0.792373</td>\n",
       "      <td>0.799163</td>\n",
       "      <td>0.789884</td>\n",
       "      <td>0.009279</td>\n",
       "      <td>0.988389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Median Impute by Titel</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.801066</td>\n",
       "      <td>0.787854</td>\n",
       "      <td>0.799520</td>\n",
       "      <td>0.785635</td>\n",
       "      <td>0.013885</td>\n",
       "      <td>0.982634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature Dropped</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.792371</td>\n",
       "      <td>0.787885</td>\n",
       "      <td>0.788776</td>\n",
       "      <td>0.783621</td>\n",
       "      <td>0.005155</td>\n",
       "      <td>0.993465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model  Train accuracy Mean  \\\n",
       "Median Impute by Pclass_&_Titel             AdaBoost             0.828843   \n",
       "Median Impute by Titel                      AdaBoost             0.827160   \n",
       "Median Impute by Pclass                     AdaBoost             0.824916   \n",
       "Feature Dropped                             AdaBoost             0.827720   \n",
       "Baseline                                    AdaBoost             0.814534   \n",
       "Median Impute by Pclass          Logistic Regression             0.807237   \n",
       "Median Impute by Pclass             Ridge Classifier             0.801627   \n",
       "Feature Dropped                     Ridge Classifier             0.797138   \n",
       "Median Impute by Pclass_&_Titel     Ridge Classifier             0.800786   \n",
       "Median Impute by Pclass_&_Titel  Logistic Regression             0.806957   \n",
       "Baseline                            Ridge Classifier             0.796297   \n",
       "Baseline                         Logistic Regression             0.798542   \n",
       "Median Impute by Titel              Ridge Classifier             0.801347   \n",
       "Median Impute by Titel           Logistic Regression             0.801066   \n",
       "Feature Dropped                  Logistic Regression             0.792371   \n",
       "\n",
       "                                 CV accuracy Mean  train_f1_Mean  CV_f1_Mean  \\\n",
       "Median Impute by Pclass_&_Titel          0.820438       0.827523    0.818651   \n",
       "Median Impute by Titel                   0.818191       0.825635    0.816315   \n",
       "Median Impute by Pclass                  0.815944       0.823637    0.815027   \n",
       "Feature Dropped                          0.808085       0.826484    0.806241   \n",
       "Baseline                                 0.805831       0.813776    0.804658   \n",
       "Median Impute by Pclass                  0.799084       0.805415    0.797176   \n",
       "Median Impute by Pclass                  0.796874       0.799391    0.794720   \n",
       "Feature Dropped                          0.796887       0.794723    0.794236   \n",
       "Median Impute by Pclass_&_Titel          0.795744       0.798787    0.793641   \n",
       "Median Impute by Pclass_&_Titel          0.795706       0.804929    0.793166   \n",
       "Baseline                                 0.793503       0.794436    0.791612   \n",
       "Baseline                                 0.791237       0.797590    0.790443   \n",
       "Median Impute by Titel                   0.792373       0.799163    0.789884   \n",
       "Median Impute by Titel                   0.787854       0.799520    0.785635   \n",
       "Feature Dropped                          0.787885       0.788776    0.783621   \n",
       "\n",
       "                                 Overfitting Score  Generalization Ratio  \n",
       "Median Impute by Pclass_&_Titel           0.008872              0.989278  \n",
       "Median Impute by Titel                    0.009320              0.988712  \n",
       "Median Impute by Pclass                   0.008610              0.989546  \n",
       "Feature Dropped                           0.020243              0.975507  \n",
       "Baseline                                  0.009117              0.988796  \n",
       "Median Impute by Pclass                   0.008239              0.989770  \n",
       "Median Impute by Pclass                   0.004672              0.994156  \n",
       "Feature Dropped                           0.000487              0.999388  \n",
       "Median Impute by Pclass_&_Titel           0.005146              0.993558  \n",
       "Median Impute by Pclass_&_Titel           0.011763              0.985386  \n",
       "Baseline                                  0.002824              0.996445  \n",
       "Baseline                                  0.007146              0.991040  \n",
       "Median Impute by Titel                    0.009279              0.988389  \n",
       "Median Impute by Titel                    0.013885              0.982634  \n",
       "Feature Dropped                           0.005155              0.993465  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0284c090-785a-44c9-b3ab-032a42494475",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mX_\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61bda33-a9ff-42bf-85d2-7f3d97a1d320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5d8a69-8078-403e-8e5f-11f44e401b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
