{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a99034f-5da7-46c5-8476-3cfb5aa24689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31e0250c-a65d-4f32-a622-562163417e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\"C:\\Users\\basde\\OneDrive\\Documenten\\GitHub\\Titanic\\train.csv\")\n",
    "test = pd.read_csv(r\"C:\\Users\\basde\\OneDrive\\Documenten\\GitHub\\Titanic\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37ad007b-5dd3-4cfd-a1eb-708c6ebb544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Sex'] = train['Sex'].map({'male': 0, 'female': 1})\n",
    "test['Sex'] = test['Sex'].map({'male': 0, 'female': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88db7be0-b64e-4057-8d05-3b6175cbaf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    int64  \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(6), object(4)\n",
      "memory usage: 83.7+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  418 non-null    int64  \n",
      " 1   Pclass       418 non-null    int64  \n",
      " 2   Name         418 non-null    object \n",
      " 3   Sex          418 non-null    int64  \n",
      " 4   Age          332 non-null    float64\n",
      " 5   SibSp        418 non-null    int64  \n",
      " 6   Parch        418 non-null    int64  \n",
      " 7   Ticket       418 non-null    object \n",
      " 8   Fare         417 non-null    float64\n",
      " 9   Cabin        91 non-null     object \n",
      " 10  Embarked     418 non-null    object \n",
      "dtypes: float64(2), int64(5), object(4)\n",
      "memory usage: 36.1+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75156529-d10b-4f4e-a827-89fe17f8bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputer.fit(train[['Fare']])\n",
    "test['Fare'] = imputer.transform(test[['Fare']])\n",
    "train['Fare'] = np.log1p(train['Fare'])\n",
    "test['Fare'] = np.log1p(test['Fare'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "359afc8b-a0f1-4cd6-9dee-0168c1831457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "def add_title_column(df):\n",
    "    \"\"\"\n",
    "    Cleans the 'Name' column and adds a 'Title' column directly \n",
    "    to the input DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to modify in-place.\n",
    "    \"\"\"\n",
    "    # 1. Clean the Name column (remove text in parentheses and quotes)\n",
    "    # This series is temporary and will be used to build the 'Title'\n",
    "    cleaned_names = df['Name'].apply(lambda x: re.sub(r'\\([^)]*\\)', '', x).strip())\n",
    "    cleaned_names = cleaned_names.str.replace(r'\"[^\"]*\"', '', regex=True).str.strip()\n",
    "\n",
    "    # 2. Extract the part of the name after the comma\n",
    "    name_part = cleaned_names.str.split(',').str.get(1)\n",
    "\n",
    "    # 3. Extract the Title from the remaining part of the name\n",
    "    extracted_title = name_part.str.split('.').str.get(0).str.strip()\n",
    "    \n",
    "    # 4. Standardize the common titles\n",
    "    title_mapping = {\n",
    "        'Mlle': 'Miss',\n",
    "        'Ms': 'Miss',\n",
    "        'Mme': 'Mrs'\n",
    "    }\n",
    "    extracted_title = extracted_title.replace(title_mapping)\n",
    "\n",
    "    # 5. Define a list of common titles\n",
    "    common_titles = ['Mr', 'Miss', 'Mrs', 'Master']\n",
    "\n",
    "    # 6. Create the 'Title' column directly on the DataFrame\n",
    "    # Categorize any title not in common_titles as 'Rare'\n",
    "    df['Titel'] = extracted_title.apply(lambda x: x if x in common_titles else 'Rare')\n",
    "    df.drop('Name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0873a20f-c269-4389-a4bd-012466aa63b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have two different dataframes: train_df and test_df\n",
    "add_title_column(train)\n",
    "add_title_column(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dd126b3-128b-4b9b-a1fd-e190e42234d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      int64\n",
       "Survived         int64\n",
       "Pclass           int64\n",
       "Sex              int64\n",
       "Age            float64\n",
       "SibSp            int64\n",
       "Parch            int64\n",
       "Ticket          object\n",
       "Fare           float64\n",
       "Cabin           object\n",
       "Embarked        object\n",
       "Titel           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "463fa68b-91dd-40e6-9038-c1963dcb3ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_validate\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9d99851-63f2-459e-955a-c07f2e07638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['Ticket', 'Cabin', 'Embarked'], axis=1 , inplace = True)\n",
    "y = train['Survived']\n",
    "train.drop(['Survived'], axis=1 , inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "597d1d74-f924-4214-8a22-8ffdd8008bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Evaluation for: Logistic Regression ---\n",
      "Evaluating: Baseline (Feature Dropped)\n",
      "Evaluating: Nominal Encoding (Ordinal)\n",
      "Evaluating: One-Hot Encoding\n",
      "Evaluating: Feature Dropped (Explicit Step)\n",
      "\n",
      "--- Evaluation Summary ---\n",
      "                            CV Score Mean  CV Score Std  Train Score Mean  \\\n",
      "One-Hot Encoding                 0.821568      0.025816          0.825199   \n",
      "Baseline                         0.788990      0.024082          0.799103   \n",
      "Feature Dropped                  0.788990      0.024082          0.799103   \n",
      "Nominal Encoding (Ordinal)       0.784483      0.034348          0.801346   \n",
      "\n",
      "                            Train Score Std  Fit Time Mean  \n",
      "One-Hot Encoding                   0.012440       0.033135  \n",
      "Baseline                           0.005552       0.015230  \n",
      "Feature Dropped                    0.005552       0.015860  \n",
      "Nominal Encoding (Ordinal)         0.008183       0.031222  \n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Starting Evaluation for: Random Forest ---\n",
      "Evaluating: Baseline (Feature Dropped)\n",
      "Evaluating: Nominal Encoding (Ordinal)\n",
      "Evaluating: One-Hot Encoding\n",
      "Evaluating: Feature Dropped (Explicit Step)\n",
      "\n",
      "--- Evaluation Summary ---\n",
      "                            CV Score Mean  CV Score Std  Train Score Mean  \\\n",
      "One-Hot Encoding                 0.832773      0.013473               1.0   \n",
      "Baseline                         0.821555      0.017111               1.0   \n",
      "Feature Dropped                  0.821555      0.017111               1.0   \n",
      "Nominal Encoding (Ordinal)       0.818185      0.019624               1.0   \n",
      "\n",
      "                            Train Score Std  Fit Time Mean  \n",
      "One-Hot Encoding                        0.0       0.478259  \n",
      "Baseline                                0.0       0.462896  \n",
      "Feature Dropped                         0.0       0.467282  \n",
      "Nominal Encoding (Ordinal)              0.0       0.488715  \n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models_to_evaluate = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000, solver='liblinear'),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# --- Main Evaluation Loop ---\n",
    "# This loop will run the entire evaluation process for each specified model.\n",
    "for model_name, model_object in models_to_evaluate.items():\n",
    "\n",
    "    print(f\"--- Starting Evaluation for: {model_name} ---\")\n",
    "\n",
    "    # 1. Setup\n",
    "    columns_to_process = ['Titel']\n",
    "    # Identify numeric columns automatically to apply imputation\n",
    "    numeric_features = train.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "    # Define the cross-validation strategy\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Define the model for this iteration\n",
    "    model = model_object\n",
    "\n",
    "    # Define the encoding techniques to be tested\n",
    "    # Each technique is a pipeline to first handle NaNs and then apply the encoding.\n",
    "    techniques = {\n",
    "        'Nominal Encoding (Ordinal)': Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "        ]),\n",
    "        'One-Hot Encoding': Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ])\n",
    "    }\n",
    "\n",
    "    # 2. Execution and Evaluation\n",
    "    results = {}\n",
    "    scoring_objective = 'accuracy' # The primary metric for evaluation\n",
    "\n",
    "    # a. Baseline Evaluation\n",
    "    # For the baseline, we will impute numeric columns and drop the categorical column.\n",
    "    # This provides a reference point for model performance without the feature.\n",
    "    print(\"Evaluating: Baseline (Feature Dropped)\")\n",
    "    baseline_train = train.drop(columns=columns_to_process)\n",
    "    baseline_numeric_features = baseline_train.select_dtypes(include=np.number).columns.tolist()\n",
    "    \n",
    "    # The baseline pipeline only needs to handle numeric imputation\n",
    "    baseline_pipeline = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    baseline_scores = cross_validate(baseline_pipeline, baseline_train, y, cv=cv,\n",
    "                                     scoring=scoring_objective, return_train_score=True)\n",
    "    results['Baseline'] = baseline_scores\n",
    "\n",
    "    # b. Technique Evaluation Loop\n",
    "    # Iterate through each defined encoding technique.\n",
    "    for name, transformer in techniques.items():\n",
    "        print(f\"Evaluating: {name}\")\n",
    "\n",
    "        # Create a preprocessor to handle numeric and categorical columns separately\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', SimpleImputer(strategy='median'), numeric_features),\n",
    "                ('cat', transformer, columns_to_process)\n",
    "            ],\n",
    "            remainder='passthrough' # Keep other columns (if any)\n",
    "        )\n",
    "\n",
    "        # Create the full pipeline: preprocess data, then train the model\n",
    "        main_pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ])\n",
    "\n",
    "        # Perform cross-validation on the full pipeline\n",
    "        scores = cross_validate(main_pipeline, train, y, cv=cv,\n",
    "                                scoring=scoring_objective, return_train_score=True)\n",
    "        results[name] = scores\n",
    "\n",
    "    # c. Feature Dropped Evaluation\n",
    "    # This step is explicitly requested. It is functionally identical to our baseline.\n",
    "    print(\"Evaluating: Feature Dropped (Explicit Step)\")\n",
    "    train_dropped = train.drop(columns=columns_to_process)\n",
    "    \n",
    "    # This pipeline is the same as the baseline pipeline\n",
    "    dropped_pipeline = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    # Perform cross-validation\n",
    "    dropped_scores = cross_validate(dropped_pipeline, train_dropped, y, cv=cv,\n",
    "                                    scoring=scoring_objective, return_train_score=True)\n",
    "    results['Feature Dropped'] = dropped_scores\n",
    "\n",
    "    # 3. Conclusion\n",
    "    # Process the results into a clean DataFrame for comparison.\n",
    "    summary = {}\n",
    "    for name, scores in results.items():\n",
    "        summary[name] = {\n",
    "            'CV Score Mean': scores['test_score'].mean(),\n",
    "            'CV Score Std': scores['test_score'].std(),\n",
    "            'Train Score Mean': scores['train_score'].mean(),\n",
    "            'Train Score Std': scores['train_score'].std(),\n",
    "            'Fit Time Mean': scores['fit_time'].mean()\n",
    "        }\n",
    "\n",
    "    results_train = pd.DataFrame.from_dict(summary, orient='index')\n",
    "    results_train = results_train.sort_values(by='CV Score Mean', ascending=False)\n",
    "\n",
    "    print(\"\\n--- Evaluation Summary ---\")\n",
    "    print(results_train)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649f3732-6901-42c3-b47d-c06a23a54844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
