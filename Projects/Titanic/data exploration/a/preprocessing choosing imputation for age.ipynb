{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a99034f-5da7-46c5-8476-3cfb5aa24689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31e0250c-a65d-4f32-a622-562163417e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\"C:\\Users\\basde\\OneDrive\\Documenten\\GitHub\\Titanic\\train.csv\")\n",
    "test = pd.read_csv(r\"C:\\Users\\basde\\OneDrive\\Documenten\\GitHub\\Titanic\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37ad007b-5dd3-4cfd-a1eb-708c6ebb544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Sex'] = train['Sex'].map({'male': 0, 'female': 1})\n",
    "test['Sex'] = test['Sex'].map({'male': 0, 'female': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e76a0f9-4caf-44b6-898e-23ce64e7a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('PassengerId', axis=1, inplace=True)\n",
    "test.drop('PassengerId', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88db7be0-b64e-4057-8d05-3b6175cbaf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 11 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Survived  891 non-null    int64  \n",
      " 1   Pclass    891 non-null    int64  \n",
      " 2   Name      891 non-null    object \n",
      " 3   Sex       891 non-null    int64  \n",
      " 4   Age       714 non-null    float64\n",
      " 5   SibSp     891 non-null    int64  \n",
      " 6   Parch     891 non-null    int64  \n",
      " 7   Ticket    891 non-null    object \n",
      " 8   Fare      891 non-null    float64\n",
      " 9   Cabin     204 non-null    object \n",
      " 10  Embarked  889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(4)\n",
      "memory usage: 76.7+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 10 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Pclass    418 non-null    int64  \n",
      " 1   Name      418 non-null    object \n",
      " 2   Sex       418 non-null    int64  \n",
      " 3   Age       332 non-null    float64\n",
      " 4   SibSp     418 non-null    int64  \n",
      " 5   Parch     418 non-null    int64  \n",
      " 6   Ticket    418 non-null    object \n",
      " 7   Fare      417 non-null    float64\n",
      " 8   Cabin     91 non-null     object \n",
      " 9   Embarked  418 non-null    object \n",
      "dtypes: float64(2), int64(4), object(4)\n",
      "memory usage: 32.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75156529-d10b-4f4e-a827-89fe17f8bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputer.fit(train[['Fare']])\n",
    "test['Fare'] = imputer.transform(test[['Fare']])\n",
    "train['Fare'] = np.log1p(train['Fare'])\n",
    "test['Fare'] = np.log1p(test['Fare'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "359afc8b-a0f1-4cd6-9dee-0168c1831457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def add_title_column(df):\n",
    "    \"\"\"\n",
    "    Cleans the 'Name' column and adds a 'Title' column directly \n",
    "    to the input DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to modify in-place.\n",
    "    \"\"\"\n",
    "    # 1. Clean the Name column (remove text in parentheses and quotes)\n",
    "    # This series is temporary and will be used to build the 'Title'\n",
    "    cleaned_names = df['Name'].apply(lambda x: re.sub(r'\\([^)]*\\)', '', x).strip())\n",
    "    cleaned_names = cleaned_names.str.replace(r'\"[^\"]*\"', '', regex=True).str.strip()\n",
    "\n",
    "    # 2. Extract the part of the name after the comma\n",
    "    name_part = cleaned_names.str.split(',').str.get(1)\n",
    "\n",
    "    # 3. Extract the Title from the remaining part of the name\n",
    "    extracted_title = name_part.str.split('.').str.get(0).str.strip()\n",
    "    \n",
    "    # 4. Standardize the common titles\n",
    "    title_mapping = {\n",
    "        'Mlle': 'Miss',\n",
    "        'Ms': 'Miss',\n",
    "        'Mme': 'Mrs'\n",
    "    }\n",
    "    extracted_title = extracted_title.replace(title_mapping)\n",
    "\n",
    "    # 5. Define a list of common titles\n",
    "    common_titles = ['Mr', 'Miss', 'Mrs', 'Master']\n",
    "\n",
    "    # 6. Create the 'Title' column directly on the DataFrame\n",
    "    # Categorize any title not in common_titles as 'Rare'\n",
    "    df['Titel'] = extracted_title.apply(lambda x: x if x in common_titles else 'Rare')\n",
    "    df.drop('Name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0873a20f-c269-4389-a4bd-012466aa63b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "add_title_column(train)\n",
    "add_title_column(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dd126b3-128b-4b9b-a1fd-e190e42234d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Titel\n",
       "Mr        240\n",
       "Miss       79\n",
       "Mrs        72\n",
       "Master     21\n",
       "Rare        6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Titel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "becfbc59-955e-4248-aec3-d0a7abdc2f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['Survived']\n",
    "train.drop(['Survived','Cabin','Embarked', 'Ticket'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ada89c16-84ac-47ad-b1ea-1db7b670ed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# Prevents wrapping by allowing unlimited horizontal width\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb0c8339-6aa6-459f-8f41-aa48394c46a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating Model: Logistic Regression ---\n",
      "\n",
      "--- Evaluating Model: Random Forest ---\n",
      "\n",
      "--- Final Comparison Across All Models ---\n",
      "\n",
      "                                               Model  CV Score Mean  \\\n",
      "Median Impute by Pclass_&_Titel  Logistic Regression         0.8013   \n",
      "Median Impute by Pclass          Logistic Regression         0.7991   \n",
      "Median Impute by Titel           Logistic Regression         0.7968   \n",
      "Baseline (Global Median)         Logistic Regression         0.7957   \n",
      "Feature Dropped                  Logistic Regression         0.7946   \n",
      "Baseline (Global Median)               Random Forest         0.8126   \n",
      "Median Impute by Pclass                Random Forest         0.8002   \n",
      "Median Impute by Pclass_&_Titel        Random Forest         0.8002   \n",
      "Feature Dropped                        Random Forest         0.7991   \n",
      "Median Impute by Titel                 Random Forest         0.7812   \n",
      "\n",
      "                                 CV Score Std  Train Score Mean  \\\n",
      "Median Impute by Pclass_&_Titel        0.0320            0.8053   \n",
      "Median Impute by Pclass                0.0266            0.8047   \n",
      "Median Impute by Titel                 0.0241            0.7997   \n",
      "Baseline (Global Median)               0.0219            0.7977   \n",
      "Feature Dropped                        0.0289            0.7918   \n",
      "Baseline (Global Median)               0.0102            0.9832   \n",
      "Median Impute by Pclass                0.0216            0.9846   \n",
      "Median Impute by Pclass_&_Titel        0.0133            0.9837   \n",
      "Feature Dropped                        0.0228            0.9270   \n",
      "Median Impute by Titel                 0.0167            0.9818   \n",
      "\n",
      "                                 Train Score Std  \n",
      "Median Impute by Pclass_&_Titel           0.0084  \n",
      "Median Impute by Pclass                   0.0082  \n",
      "Median Impute by Titel                    0.0053  \n",
      "Baseline (Global Median)                  0.0051  \n",
      "Feature Dropped                           0.0128  \n",
      "Baseline (Global Median)                  0.0028  \n",
      "Median Impute by Pclass                   0.0028  \n",
      "Median Impute by Pclass_&_Titel           0.0036  \n",
      "Feature Dropped                           0.0027  \n",
      "Median Impute by Titel                    0.0024  \n"
     ]
    }
   ],
   "source": [
    "class GroupedMedianImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A custom transformer to impute missing values in 'Age' based on the\n",
    "    median of groups. It expects 'Title' to be pre-calculated.\n",
    "    \"\"\"\n",
    "    def __init__(self, group_cols):\n",
    "        self.group_cols = group_cols\n",
    "        self.medians = {}\n",
    "        self.global_median = 0\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_ = X.copy()\n",
    "        # Calculate medians based on the specified grouping columns\n",
    "        if self.group_cols:\n",
    "            self.medians = X_.groupby(self.group_cols)['Age'].median()\n",
    "        self.global_median = X_['Age'].median()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        \n",
    "        # Impute 'Age'\n",
    "        if self.group_cols:\n",
    "            X_['Age'] = X_.groupby(self.group_cols)['Age'].transform(lambda x: x.fillna(x.median()))\n",
    "        \n",
    "        # Fill any remaining NaNs (for groups that might not be in the test set)\n",
    "        X_['Age'] = X_['Age'].fillna(self.global_median)\n",
    "        \n",
    "        # Drop non-numeric columns used for grouping to prepare data for the model\n",
    "        X_ = X_.select_dtypes(include=np.number)\n",
    "        return X_\n",
    "\n",
    "# 1. Imports are handled above\n",
    "\n",
    "# 2. Setup\n",
    "# a. Define columns to process\n",
    "columns_to_process = ['Age']\n",
    "\n",
    "# b. Define cross-validation strategy\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# c. Define evaluation models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=2000, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# d. Define techniques dictionary\n",
    "techniques = {}\n",
    "grouping_features = ['Pclass', 'Titel'] \n",
    "for i in range(1, len(grouping_features) + 1):\n",
    "    for combo in itertools.combinations(grouping_features, i):\n",
    "        combo_list = list(combo)\n",
    "        name = f\"Median Impute by {'_&_'.join(combo_list)}\"\n",
    "        techniques[name] = GroupedMedianImputer(group_cols=combo_list)\n",
    "\n",
    "# 3. Execution and Evaluation\n",
    "# This dictionary will hold all results for final comparison\n",
    "all_results_train = pd.DataFrame()\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"--- Evaluating Model: {model_name} ---\\n\")\n",
    "    results = {}\n",
    "    \n",
    "    # Create a numeric-only version for baseline and feature-dropped scenarios\n",
    "    train_numeric = train.select_dtypes(include=np.number)\n",
    "    \n",
    "    # b. Baseline Evaluation\n",
    "    # For a fair baseline, we'll impute 'Age' with the global median\n",
    "    train_baseline = train_numeric.copy()\n",
    "    train_baseline['Age'] = train_baseline['Age'].fillna(train_numeric['Age'].median())\n",
    "    \n",
    "    baseline_scores = cross_validate(model, train_baseline, y, cv=cv, scoring='accuracy', return_train_score=True)\n",
    "    results['Baseline (Global Median)'] = {\n",
    "        'Train Score Mean': baseline_scores['train_score'].mean(),\n",
    "        'Train Score Std': baseline_scores['train_score'].std(),\n",
    "        'CV Score Mean': baseline_scores['test_score'].mean(),\n",
    "        'CV Score Std': baseline_scores['test_score'].std()\n",
    "    }\n",
    "\n",
    "    # c. Technique Evaluation Loop\n",
    "    for name, imputer in techniques.items():\n",
    "        # The imputer is the first step, it receives the train with 'Title' and outputs a numeric train\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('imputer', imputer),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        \n",
    "        # Perform cross-validation on the pipeline\n",
    "        # The full train (with 'Title') is passed to the pipeline\n",
    "        scores = cross_validate(pipeline, train, y, cv=cv, scoring='accuracy', return_train_score=True)\n",
    "        \n",
    "        results[name] = {\n",
    "            'Train Score Mean': scores['train_score'].mean(),\n",
    "            'Train Score Std': scores['train_score'].std(),\n",
    "            'CV Score Mean': scores['test_score'].mean(),\n",
    "            'CV Score Std': scores['test_score'].std()\n",
    "        }\n",
    "\n",
    "    # d. Feature Dropped Evaluation\n",
    "    train_dropped = train_numeric.drop(columns=columns_to_process)\n",
    "    dropped_scores = cross_validate(model, train_dropped, y, cv=cv, scoring='accuracy', return_train_score=True)\n",
    "    results['Feature Dropped'] = {\n",
    "        'Train Score Mean': dropped_scores['train_score'].mean(),\n",
    "        'Train Score Std': dropped_scores['train_score'].std(),\n",
    "        'CV Score Mean': dropped_scores['test_score'].mean(),\n",
    "        'CV Score Std': dropped_scores['test_score'].std()\n",
    "    }\n",
    "\n",
    "    # 4. Conclusion for the current model\n",
    "    # Convert results to a DataFrame for clear comparison\n",
    "    model_results_train = pd.DataFrame.from_dict(results, orient='index')\n",
    "    model_results_train['Model'] = model_name\n",
    "    model_results_train = model_results_train.sort_values(by='CV Score Mean', ascending=False)\n",
    "    all_results_train = pd.concat([all_results_train, model_results_train])\n",
    "\n",
    "\n",
    "# Final Conclusion: Print all results\n",
    "print(\"--- Final Comparison Across All Models ---\\n\")\n",
    "# Format the output for better readability\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "print(all_results_train[['Model', 'CV Score Mean', 'CV Score Std', 'Train Score Mean', 'Train Score Std']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463fa68b-91dd-40e6-9038-c1963dcb3ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ac9bb9-e330-4e8e-95af-793f5f846cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_encode = 'Titel' \n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# 2. Fit and transform the data\n",
    "# The double brackets around the column name create a DataFrame slice, which is the expected input format.\n",
    "one_hot_encoded = ohe.fit_transform(train[[column_to_encode]])\n",
    "\n",
    "# 3. Create a new DataFrame with the one-hot encoded columns\n",
    "# The `get_feature_names_out()` method provides meaningful names for the new columns.\n",
    "one_hot_df = pd.DataFrame(one_hot_encoded, columns=ohe.get_feature_names_out([column_to_encode]))\n",
    "\n",
    "# 4. Concatenate the new DataFrame with the original DataFrame\n",
    "# We use `train.index` to ensure the rows align correctly.\n",
    "train = pd.concat([train, one_hot_df], axis=1)\n",
    "\n",
    "# 5. Drop the original column\n",
    "train = train.drop(column_to_encode, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ee04d3-16a3-42c7-a38f-11a4cbd48221",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded = ohe.transform(test[[column_to_encode]])\n",
    "\n",
    "# 3. Create a new DataFrame with the one-hot encoded columns\n",
    "# The `get_feature_names_out()` method provides meaningful names for the new columns.\n",
    "one_hot_df = pd.DataFrame(one_hot_encoded, columns=ohe.get_feature_names_out([column_to_encode]))\n",
    "\n",
    "# 4. Concatenate the new DataFrame with the original DataFrame\n",
    "# We use `test.index` to ensure the rows align correctly.\n",
    "test = pd.concat([test, one_hot_df], axis=1)\n",
    "\n",
    "# 5. Drop the original column\n",
    "test = test.drop(column_to_encode, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07b0dcf-d9e3-48f7-9662-1e1a42d51f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb701317-4862-4cb0-9893-ba00883e1b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca638df-abe7-4788-9c37-d9beba794337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b69362-9985-4315-ac11-22bb63a1cc59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf27846-c740-4366-a340-222460a8f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import itertools\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602b9486-910e-472f-9feb-9d65a61e2d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e61a62-99c9-4cd0-a568-814dc0908389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8ef082-84a6-4493-bb66-2d295cc1eaa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac4bda0-d0ea-4651-ab5e-50c7139dec9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
