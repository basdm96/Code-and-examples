{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a83ca33-c4af-44d0-bfc1-2e77018dc26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from sklearn.model_selection import KFold, cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4778aecf-73cb-4404-8cdc-01041192801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classification dataset\n",
    "dft = pd.read_csv(r\"C:\\Users\\basde\\Documents\\GitHub\\Code-and-examples\\Projects\\Binary Classification with a Bank Dataset\\train.csv\")\n",
    "dfo = pd.read_csv(r\"C:\\Users\\basde\\Documents\\GitHub\\Code-and-examples\\Projects\\Binary Classification with a Bank Dataset\\bank-full.csv\", delimiter=';')\n",
    "dfo['y'] = dfo['y'].map({'no' : 0, 'yes': 1})\n",
    "# df = pd.concat([dft, dfo])\n",
    "# y = dfo['y']\n",
    "# dfo = dfo.drop('y', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80419a82-90ce-450f-ab82-5505c7767502",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo['default'] = pd.get_dummies(dfo['default'], drop_first=True, dtype=int)\n",
    "dfo['housing'] = pd.get_dummies(dfo['housing'], drop_first=True, dtype=int)\n",
    "dfo['loan'] = pd.get_dummies(dfo['loan'], drop_first=True, dtype=int)\n",
    "dfo['contact'] = dfo['contact'].map({'telephone': 'cellular', 'unknown' : 'unknown', 'telephone': 'telephone'})\n",
    "dfo['contact'] = pd.get_dummies(dfo['contact'], drop_first=True, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23f07bcd-ec39-4310-b091-7229ed2115e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age           int64\n",
       "job          object\n",
       "marital      object\n",
       "education    object\n",
       "default       int64\n",
       "balance       int64\n",
       "housing       int64\n",
       "loan          int64\n",
       "contact       int64\n",
       "day           int64\n",
       "month        object\n",
       "duration      int64\n",
       "campaign      int64\n",
       "pdays         int64\n",
       "previous      int64\n",
       "poutcome     object\n",
       "y             int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfo.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603b8ff3-9b7c-4280-93d3-027588796720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "504eac16-6081-47a6-a589-ef73aa9cdc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# This cell assumes a DataFrame 'dfo' exists with 'month', 'day', and 'y' columns.\n",
    "# It prepares the data and engineers features for modeling.\n",
    "\n",
    "# Ensure all object columns are converted to 'category' dtype for native handling by LightGBM\n",
    "for col in dfo.select_dtypes(include=['object']).columns:\n",
    "    if col != 'y': \n",
    "        dfo[col] = dfo[col].astype('category')\n",
    "\n",
    "# Create a mapping from month name to month number\n",
    "month_map = {\n",
    "    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,\n",
    "    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "}\n",
    "dfo['month_num'] = dfo['month'].map(month_map)\n",
    "\n",
    "# --- FIX for 'day is out of range for month' error ---\n",
    "# Create a map of days per month and cap the 'day' column to a valid day.\n",
    "# This handles invalid dates like April 31st or February 30th.\n",
    "days_in_month = {1: 31, 2: 28, 3: 31, 4: 30, 5: 31, 6: 30, 7: 31, 8: 31, 9: 30, 10: 31, 11: 30, 12: 31}\n",
    "dfo['day_corrected'] = dfo.apply(lambda row: min(row['day'], days_in_month[row['month_num']]), axis=1)\n",
    "\n",
    "# Create a proper datetime column using the corrected day\n",
    "dfo['date'] = pd.to_datetime('2023-' + dfo['month_num'].astype(str) + '-' + dfo['day_corrected'].astype(str))\n",
    "# --- End of fix ---\n",
    "\n",
    "# Engineer a categorical feature 'month_day_bin' using the original day\n",
    "bins = [0, 5, 10, 15, 20, 25, 31]\n",
    "labels = ['1-5', '6-10', '11-15', '16-20', '21-25', '26-31']\n",
    "dfo['day_bin'] = pd.cut(dfo['day'], bins=bins, labels=labels, right=True).astype(str)\n",
    "dfo['month_day_bin'] = dfo['month'].astype(str) + '_' + dfo['day_bin']\n",
    "dfo['month_day_bin'] = dfo['month_day_bin'].astype('category')\n",
    "\n",
    "# Engineer 'week_of_year' feature\n",
    "dfo['week_of_year'] = dfo['date'].dt.isocalendar().week\n",
    "\n",
    "# Engineer cyclical features for the week of the year\n",
    "dfo['week_sin'] = np.sin(2 * np.pi * dfo['week_of_year'] / 52)\n",
    "dfo['week_cos'] = np.cos(2 * np.pi * dfo['week_of_year'] / 52)\n",
    "\n",
    "# Define the final feature matrix X and target vector y\n",
    "# Drop intermediate columns used for feature creation\n",
    "feature_cols = [\n",
    "    'month', 'day', 'month_day_bin', 'week_of_year', 'week_sin', 'week_cos'\n",
    "]\n",
    "other_cols = [col for col in dfo.columns if col not in ['y', 'month_num', 'date', 'day_bin', 'day_corrected'] and col not in feature_cols]\n",
    "X = dfo[feature_cols + other_cols]\n",
    "y = dfo['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a57e6f-9253-45da-9061-3aff95b21f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e578a996-a44c-4140-8d89-ea76c15164c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "month            category\n",
       "day                 int64\n",
       "month_day_bin    category\n",
       "week_of_year       UInt32\n",
       "week_sin          Float64\n",
       "week_cos          Float64\n",
       "age                 int64\n",
       "job              category\n",
       "marital          category\n",
       "education        category\n",
       "default             int64\n",
       "balance             int64\n",
       "housing             int64\n",
       "loan                int64\n",
       "contact             int64\n",
       "duration            int64\n",
       "campaign            int64\n",
       "pdays               int64\n",
       "previous            int64\n",
       "poutcome         category\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29e4e136-5974-4ba6-985c-335623f2133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, TargetEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# a. Specify the model to evaluate\n",
    "models = {\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# b. Define the transformation pipelines to test\n",
    "# Each pipeline is a ColumnTransformer. We set verbose_feature_names_out=False\n",
    "# to keep original column names where possible.\n",
    "transformation_pipelines = {\n",
    "    'Ordinal_with_originals': ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), ['month_day_bin']),\n",
    "            ('drop_features', 'drop', ['week_of_year', 'week_sin', 'week_cos'])\n",
    "        ],\n",
    "        remainder='passthrough',\n",
    "        verbose_feature_names_out=False\n",
    "    ),\n",
    "    'Ordinal_without_originals': ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), ['month_day_bin']),\n",
    "            ('drop_features', 'drop', ['month', 'day', 'week_of_year', 'week_sin', 'week_cos'])\n",
    "        ],\n",
    "        remainder='passthrough',\n",
    "        verbose_feature_names_out=False\n",
    "    ),\n",
    "    'TargetEnc_with_originals': ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('target', TargetEncoder(target_type='binary'), ['week_of_year']),\n",
    "            ('drop_features', 'drop', ['month_day_bin', 'week_sin', 'week_cos'])\n",
    "        ],\n",
    "        remainder='passthrough',\n",
    "        verbose_feature_names_out=False\n",
    "    ),\n",
    "    'TargetEnc_without_originals': ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('target', TargetEncoder(target_type='binary'), ['week_of_year']),\n",
    "            ('drop_features', 'drop', ['month', 'day', 'month_day_bin', 'week_sin', 'week_cos'])\n",
    "        ],\n",
    "        remainder='passthrough',\n",
    "        verbose_feature_names_out=False\n",
    "    ),\n",
    "    'Cyclical_with_originals': ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('scaler', StandardScaler(), ['week_sin', 'week_cos']),\n",
    "            ('drop_features', 'drop', ['month_day_bin', 'week_of_year'])\n",
    "        ],\n",
    "        remainder='passthrough',\n",
    "        verbose_feature_names_out=False\n",
    "    ),\n",
    "    'Cyclical_without_originals': ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('scaler', StandardScaler(), ['week_sin', 'week_cos']),\n",
    "            ('drop_features', 'drop', ['month', 'day', 'month_day_bin', 'week_of_year'])\n",
    "        ],\n",
    "        remainder='passthrough',\n",
    "        verbose_feature_names_out=False\n",
    "    )\n",
    "}\n",
    "\n",
    "# CRITICAL FIX: Set the output of all transformers to be a pandas DataFrame.\n",
    "# This preserves the 'category' dtype so LightGBM can handle it natively.\n",
    "for preprocessor in transformation_pipelines.values():\n",
    "    preprocessor.set_output(transform=\"pandas\")\n",
    "\n",
    "\n",
    "# c. Define the classification scoring metrics\n",
    "scoring_metrics = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1_score': 'f1_weighted',\n",
    "}\n",
    "\n",
    "# d. Define the cross-validation strategy\n",
    "cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee313226-1297-4617-8458-360f2ad410b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, TargetEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# a. Specify the model to evaluate\n",
    "models = {\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# b. Define the transformation pipelines to test\n",
    "# Each pipeline is a ColumnTransformer. We set verbose_feature_names_out=False\n",
    "# to keep original column names where possible.\n",
    "transformation_pipelines = {\n",
    "    'Ordinal_with_originals': ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), ['month_day_bin']),\n",
    "            ('drop_features', 'drop', ['week_of_year', 'week_sin', 'week_cos'])\n",
    "        ],\n",
    "        remainder='passthrough',\n",
    "        verbose_feature_names_out=False\n",
    "    ),\n",
    "    'Ordinal_without_originals': ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), ['month_day_bin']),\n",
    "            ('drop_features', 'drop', ['month', 'day', 'week_of_year', 'week_sin', 'week_cos'])\n",
    "        ],\n",
    "        remainder='passthrough',\n",
    "        verbose_feature_names_out=False\n",
    "    ),\n",
    "    'TargetEnc_with_originals': ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('target', TargetEncoder(target_type='binary'), ['week_of_year']),\n",
    "            ('drop_features', 'drop', ['month_day_bin', 'week_sin', 'week_cos'])\n",
    "        ],\n",
    "        remainder='passthrough',\n",
    "        verbose_feature_names_out=False\n",
    "    ),\n",
    "    'TargetEnc_without_originals': ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('target', TargetEncoder(target_type='binary'), ['week_of_year']),\n",
    "            ('drop_features', 'drop', ['month', 'day', 'month_day_bin', 'week_sin', 'week_cos'])\n",
    "        ],\n",
    "        remainder='passthrough',\n",
    "        verbose_feature_names_out=False\n",
    "    ),\n",
    "    'Cyclical_with_originals': ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('scaler', StandardScaler(), ['week_sin', 'week_cos']),\n",
    "            ('drop_features', 'drop', ['month_day_bin', 'week_of_year'])\n",
    "        ],\n",
    "        remainder='passthrough',\n",
    "        verbose_feature_names_out=False\n",
    "    ),\n",
    "    'Cyclical_without_originals': ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('scaler', StandardScaler(), ['week_sin', 'week_cos']),\n",
    "            ('drop_features', 'drop', ['month', 'day', 'month_day_bin', 'week_of_year'])\n",
    "        ],\n",
    "        remainder='passthrough',\n",
    "        verbose_feature_names_out=False\n",
    "    )\n",
    "}\n",
    "\n",
    "# CRITICAL FIX: Set the output of all transformers to be a pandas DataFrame.\n",
    "# This preserves the 'category' dtype so LightGBM can handle it natively.\n",
    "for preprocessor in transformation_pipelines.values():\n",
    "    preprocessor.set_output(transform=\"pandas\")\n",
    "\n",
    "\n",
    "# c. Define the classification scoring metrics\n",
    "scoring_metrics = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1_score': 'f1_weighted',\n",
    "}\n",
    "\n",
    "# d. Define the cross-validation strategy\n",
    "cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c293cf4-e09f-4180-b5a0-cd919777ce60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating Model: LightGBM ---\n",
      "[LightGBM] [Info] Number of positive: 4198, number of negative: 31970\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1062\n",
      "[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116069 -> initscore=-2.030190\n",
      "[LightGBM] [Info] Start training from score -2.030190\n",
      "[LightGBM] [Info] Number of positive: 4279, number of negative: 31890\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005681 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1063\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.118306 -> initscore=-2.008573\n",
      "[LightGBM] [Info] Start training from score -2.008573\n",
      "[LightGBM] [Info] Number of positive: 4184, number of negative: 31985\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005876 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1061\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.115679 -> initscore=-2.033999\n",
      "[LightGBM] [Info] Start training from score -2.033999\n",
      "[LightGBM] [Info] Number of positive: 4269, number of negative: 31900\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006131 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1061\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.118029 -> initscore=-2.011226\n",
      "[LightGBM] [Info] Start training from score -2.011226\n",
      "[LightGBM] [Info] Number of positive: 4226, number of negative: 31943\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004634 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1063\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116840 -> initscore=-2.022697\n",
      "[LightGBM] [Info] Start training from score -2.022697\n",
      "--- Evaluated transformation: Ordinal_with_originals ---\n",
      "[LightGBM] [Info] Number of positive: 4198, number of negative: 31970\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005691 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1018\n",
      "[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116069 -> initscore=-2.030190\n",
      "[LightGBM] [Info] Start training from score -2.030190\n",
      "[LightGBM] [Info] Number of positive: 4279, number of negative: 31890\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004886 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1019\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.118306 -> initscore=-2.008573\n",
      "[LightGBM] [Info] Start training from score -2.008573\n",
      "[LightGBM] [Info] Number of positive: 4184, number of negative: 31985\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007836 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1017\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.115679 -> initscore=-2.033999\n",
      "[LightGBM] [Info] Start training from score -2.033999\n",
      "[LightGBM] [Info] Number of positive: 4269, number of negative: 31900\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004858 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1017\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.118029 -> initscore=-2.011226\n",
      "[LightGBM] [Info] Start training from score -2.011226\n",
      "[LightGBM] [Info] Number of positive: 4226, number of negative: 31943\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005178 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1019\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116840 -> initscore=-2.022697\n",
      "[LightGBM] [Info] Start training from score -2.022697\n",
      "--- Evaluated transformation: Ordinal_without_originals ---\n",
      "[LightGBM] [Info] Number of positive: 4198, number of negative: 31970\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008842 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1209\n",
      "[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116069 -> initscore=-2.030190\n",
      "[LightGBM] [Info] Start training from score -2.030190\n",
      "[LightGBM] [Info] Number of positive: 4279, number of negative: 31890\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003567 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1243\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.118306 -> initscore=-2.008573\n",
      "[LightGBM] [Info] Start training from score -2.008573\n",
      "[LightGBM] [Info] Number of positive: 4184, number of negative: 31985\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1205\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.115679 -> initscore=-2.033999\n",
      "[LightGBM] [Info] Start training from score -2.033999\n",
      "[LightGBM] [Info] Number of positive: 4269, number of negative: 31900\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004566 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1239\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.118029 -> initscore=-2.011226\n",
      "[LightGBM] [Info] Start training from score -2.011226\n",
      "[LightGBM] [Info] Number of positive: 4226, number of negative: 31943\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003842 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1209\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116840 -> initscore=-2.022697\n",
      "[LightGBM] [Info] Start training from score -2.022697\n",
      "--- Evaluated transformation: TargetEnc_with_originals ---\n",
      "[LightGBM] [Info] Number of positive: 4198, number of negative: 31970\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004939 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1164\n",
      "[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116069 -> initscore=-2.030190\n",
      "[LightGBM] [Info] Start training from score -2.030190\n",
      "[LightGBM] [Info] Number of positive: 4279, number of negative: 31890\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003580 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1164\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.118306 -> initscore=-2.008573\n",
      "[LightGBM] [Info] Start training from score -2.008573\n",
      "[LightGBM] [Info] Number of positive: 4184, number of negative: 31985\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003896 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1161\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.115679 -> initscore=-2.033999\n",
      "[LightGBM] [Info] Start training from score -2.033999\n",
      "[LightGBM] [Info] Number of positive: 4269, number of negative: 31900\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1194\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.118029 -> initscore=-2.011226\n",
      "[LightGBM] [Info] Start training from score -2.011226\n",
      "[LightGBM] [Info] Number of positive: 4226, number of negative: 31943\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1163\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116840 -> initscore=-2.022697\n",
      "[LightGBM] [Info] Start training from score -2.022697\n",
      "--- Evaluated transformation: TargetEnc_without_originals ---\n",
      "[LightGBM] [Info] Number of positive: 4198, number of negative: 31970\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004122 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1076\n",
      "[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116069 -> initscore=-2.030190\n",
      "[LightGBM] [Info] Start training from score -2.030190\n",
      "[LightGBM] [Info] Number of positive: 4279, number of negative: 31890\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003877 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1077\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.118306 -> initscore=-2.008573\n",
      "[LightGBM] [Info] Start training from score -2.008573\n",
      "[LightGBM] [Info] Number of positive: 4184, number of negative: 31985\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005012 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1073\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.115679 -> initscore=-2.033999\n",
      "[LightGBM] [Info] Start training from score -2.033999\n",
      "[LightGBM] [Info] Number of positive: 4269, number of negative: 31900\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1077\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.118029 -> initscore=-2.011226\n",
      "[LightGBM] [Info] Start training from score -2.011226\n",
      "[LightGBM] [Info] Number of positive: 4226, number of negative: 31943\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1077\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116840 -> initscore=-2.022697\n",
      "[LightGBM] [Info] Start training from score -2.022697\n",
      "--- Evaluated transformation: Cyclical_with_originals ---\n",
      "[LightGBM] [Info] Number of positive: 4198, number of negative: 31970\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003142 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1032\n",
      "[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116069 -> initscore=-2.030190\n",
      "[LightGBM] [Info] Start training from score -2.030190\n",
      "[LightGBM] [Info] Number of positive: 4279, number of negative: 31890\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004838 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1033\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.118306 -> initscore=-2.008573\n",
      "[LightGBM] [Info] Start training from score -2.008573\n",
      "[LightGBM] [Info] Number of positive: 4184, number of negative: 31985\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1029\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.115679 -> initscore=-2.033999\n",
      "[LightGBM] [Info] Start training from score -2.033999\n",
      "[LightGBM] [Info] Number of positive: 4269, number of negative: 31900\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007111 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1033\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.118029 -> initscore=-2.011226\n",
      "[LightGBM] [Info] Start training from score -2.011226\n",
      "[LightGBM] [Info] Number of positive: 4226, number of negative: 31943\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1033\n",
      "[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116840 -> initscore=-2.022697\n",
      "[LightGBM] [Info] Start training from score -2.022697\n",
      "--- Evaluated transformation: Cyclical_without_originals ---\n",
      "\n",
      "--- Final Results ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Preprocessing Technique</th>\n",
       "      <th>Train F1-Score</th>\n",
       "      <th>CV F1-Score</th>\n",
       "      <th>CV Accuracy</th>\n",
       "      <th>Model</th>\n",
       "      <th>Generalization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TargetEnc_with_originals</td>\n",
       "      <td>0.929322</td>\n",
       "      <td>0.905231</td>\n",
       "      <td>0.909668</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.974076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cyclical_with_originals</td>\n",
       "      <td>0.931550</td>\n",
       "      <td>0.904998</td>\n",
       "      <td>0.909248</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.971497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ordinal_with_originals</td>\n",
       "      <td>0.930589</td>\n",
       "      <td>0.904383</td>\n",
       "      <td>0.908739</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.971839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Cyclical_without_originals</td>\n",
       "      <td>0.928966</td>\n",
       "      <td>0.903745</td>\n",
       "      <td>0.908673</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.972851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ordinal_without_originals</td>\n",
       "      <td>0.927519</td>\n",
       "      <td>0.902551</td>\n",
       "      <td>0.908120</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.973081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TargetEnc_without_originals</td>\n",
       "      <td>0.922782</td>\n",
       "      <td>0.902387</td>\n",
       "      <td>0.907987</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.977898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Preprocessing Technique  Train F1-Score  CV F1-Score  CV Accuracy  \\\n",
       "2     TargetEnc_with_originals        0.929322     0.905231     0.909668   \n",
       "4      Cyclical_with_originals        0.931550     0.904998     0.909248   \n",
       "0       Ordinal_with_originals        0.930589     0.904383     0.908739   \n",
       "5   Cyclical_without_originals        0.928966     0.903745     0.908673   \n",
       "1    Ordinal_without_originals        0.927519     0.902551     0.908120   \n",
       "3  TargetEnc_without_originals        0.922782     0.902387     0.907987   \n",
       "\n",
       "      Model  Generalization  \n",
       "2  LightGBM        0.974076  \n",
       "4  LightGBM        0.971497  \n",
       "0  LightGBM        0.971839  \n",
       "5  LightGBM        0.972851  \n",
       "1  LightGBM        0.973081  \n",
       "3  LightGBM        0.977898  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This DataFrame will hold all results for final comparison\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# --- Main Loop ---\n",
    "# Iterates through the models defined in the previous cell\n",
    "for model_name, model in models.items():\n",
    "    print(f\"--- Evaluating Model: {model_name} ---\")\n",
    "    model_results = {}\n",
    "\n",
    "    # Iterates through the ColumnTransformer pipelines to evaluate each feature engineering technique\n",
    "    for tech_name, preprocessor in transformation_pipelines.items():\n",
    "        # Create a full pipeline that first preprocesses the data and then applies the model\n",
    "        full_pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        scores = cross_validate(\n",
    "            full_pipeline, X, y, cv=cv_strategy,\n",
    "            scoring=scoring_metrics, return_train_score=True\n",
    "        )\n",
    "        \n",
    "        # Store the mean scores for the current technique\n",
    "        model_results[tech_name] = {\n",
    "            'Train F1-Score': scores['train_f1_score'].mean(),\n",
    "            'CV F1-Score': scores['test_f1_score'].mean(),\n",
    "            'CV Accuracy': scores['test_accuracy'].mean()\n",
    "        }\n",
    "        print(f\"--- Evaluated transformation: {tech_name} ---\")\n",
    "\n",
    "    # Consolidate and store results for the current model\n",
    "    temp_df = pd.DataFrame.from_dict(model_results, orient='index')\n",
    "    temp_df['Model'] = model_name\n",
    "    all_results = pd.concat([all_results, temp_df])\n",
    "\n",
    "# Final processing for the results table\n",
    "all_results.reset_index(inplace=True)\n",
    "all_results.rename(columns={'index': 'Preprocessing Technique'}, inplace=True)\n",
    "# Calculate a generalization score to check for overfitting\n",
    "all_results['Generalization'] = all_results['CV F1-Score'] / all_results['Train F1-Score']\n",
    "# Sort the results by the cross-validated F1-score for easy comparison\n",
    "all_results = all_results.sort_values(by='CV F1-Score', ascending=False)\n",
    "\n",
    "# Display the final comparative results\n",
    "print(\"\\n--- Final Results ---\")\n",
    "display(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "488e7202-c308-46ad-8035-3a282447c830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing Technique</th>\n",
       "      <th>CV F1-Score</th>\n",
       "      <th>CV Accuracy</th>\n",
       "      <th>Train F1-Score</th>\n",
       "      <th>Generalization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>TargetEnc_with_originals</td>\n",
       "      <td>0.905231</td>\n",
       "      <td>0.909668</td>\n",
       "      <td>0.929322</td>\n",
       "      <td>0.974076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>Cyclical_with_originals</td>\n",
       "      <td>0.904998</td>\n",
       "      <td>0.909248</td>\n",
       "      <td>0.931550</td>\n",
       "      <td>0.971497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>Ordinal_with_originals</td>\n",
       "      <td>0.904383</td>\n",
       "      <td>0.908739</td>\n",
       "      <td>0.930589</td>\n",
       "      <td>0.971839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>Cyclical_without_originals</td>\n",
       "      <td>0.903745</td>\n",
       "      <td>0.908673</td>\n",
       "      <td>0.928966</td>\n",
       "      <td>0.972851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>Ordinal_without_originals</td>\n",
       "      <td>0.902551</td>\n",
       "      <td>0.908120</td>\n",
       "      <td>0.927519</td>\n",
       "      <td>0.973081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>TargetEnc_without_originals</td>\n",
       "      <td>0.902387</td>\n",
       "      <td>0.907987</td>\n",
       "      <td>0.922782</td>\n",
       "      <td>0.977898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model      Preprocessing Technique  CV F1-Score  CV Accuracy  \\\n",
       "2  LightGBM     TargetEnc_with_originals     0.905231     0.909668   \n",
       "4  LightGBM      Cyclical_with_originals     0.904998     0.909248   \n",
       "0  LightGBM       Ordinal_with_originals     0.904383     0.908739   \n",
       "5  LightGBM   Cyclical_without_originals     0.903745     0.908673   \n",
       "1  LightGBM    Ordinal_without_originals     0.902551     0.908120   \n",
       "3  LightGBM  TargetEnc_without_originals     0.902387     0.907987   \n",
       "\n",
       "   Train F1-Score  Generalization  \n",
       "2        0.929322        0.974076  \n",
       "4        0.931550        0.971497  \n",
       "0        0.930589        0.971839  \n",
       "5        0.928966        0.972851  \n",
       "1        0.927519        0.973081  \n",
       "3        0.922782        0.977898  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reorder columns for a more logical presentation\n",
    "final_columns_order = [\n",
    "    'Model',\n",
    "    'Preprocessing Technique',\n",
    "    'CV F1-Score',\n",
    "    'CV Accuracy',\n",
    "    'Train F1-Score',\n",
    "    'Generalization'\n",
    "]\n",
    "all_results = all_results[final_columns_order]\n",
    "\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be208fa-ad43-40a2-9f48-6a245707584d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
